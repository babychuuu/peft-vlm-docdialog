{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtqy8--Aa9i8",
        "outputId": "638af4d0-1719-4522-cae0-41f9766a3717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在加载数据: /content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/sharegpttest/generated_predictions.jsonl\n",
            "加载了 548 条数据\n",
            "未能提取任何预测结果\n"
          ]
        }
      ],
      "source": [
        "# 直接在代码中运行\n",
        "input_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/sharegpttest/generated_predictions.jsonl\"\n",
        "results = evaluate_model(input_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xmp_hTFXcClv",
        "outputId": "c202a75b-4c08-4d11-c3f3-53c60d0de38c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在加载数据: /content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/sharegpttest/generated_predictions.jsonl\n",
            "加载了 548 条数据\n",
            "\n",
            "=== 数据格式检查 ===\n",
            "第一个样本的字段:\n",
            "  prompt: '<|im_start|>system\n",
            "You are a helpful assistant.<|i...'\n",
            "  predict: 'Tony\n",
            "Tonyassistant\n",
            "Tony Robbins, in his book \"The ...'\n",
            "  label: 'Tony Robbins describes six core human needs that d...'\n",
            "成功提取 548 条样本\n",
            "预测样例: Tony\n",
            "Tonyassistant\n",
            "Tony Robbins, in his book \"The 6 Human Needs,\" explains the 6 human needs that ar...\n",
            "真实标签样例: Tony Robbins describes six core human needs that drive our behaviors and motivations. These six need...\n",
            "\n",
            "正在计算评估指标...\n",
            "\n",
            "=== 评估结果 ===\n",
            "样本数量: 548\n",
            "EM Score: 0.0000 (0.00%)\n",
            "F1 Score: 0.2642 (26.42%)\n",
            "ANLS Score: 0.0282 (2.82%)\n",
            "Reranker Score: 0.0000 (0.00%)\n",
            "\n",
            "结果已保存到: evaluation_results.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "class EvaluationMetrics:\n",
        "    \"\"\"计算各种NLP评估指标的类\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_answer(s: str) -> str:\n",
        "        \"\"\"标准化答案文本\"\"\"\n",
        "        def remove_articles(text):\n",
        "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "        def white_space_fix(text):\n",
        "            return ' '.join(text.split())\n",
        "\n",
        "        def remove_punc(text):\n",
        "            exclude = set(string.punctuation)\n",
        "            return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "        def lower(text):\n",
        "            return text.lower()\n",
        "\n",
        "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "    @staticmethod\n",
        "    def exact_match(prediction: str, ground_truth: str) -> float:\n",
        "        \"\"\"计算精确匹配(EM)分数\"\"\"\n",
        "        return float(EvaluationMetrics.normalize_answer(prediction) ==\n",
        "                    EvaluationMetrics.normalize_answer(ground_truth))\n",
        "\n",
        "    @staticmethod\n",
        "    def f1_score(prediction: str, ground_truth: str) -> float:\n",
        "        \"\"\"计算F1分数\"\"\"\n",
        "        pred_tokens = EvaluationMetrics.normalize_answer(prediction).split()\n",
        "        truth_tokens = EvaluationMetrics.normalize_answer(ground_truth).split()\n",
        "\n",
        "        if len(pred_tokens) == 0 and len(truth_tokens) == 0:\n",
        "            return 1.0\n",
        "        if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        common = Counter(pred_tokens) & Counter(truth_tokens)\n",
        "        num_same = sum(common.values())\n",
        "\n",
        "        precision = num_same / len(pred_tokens)\n",
        "        recall = num_same / len(truth_tokens)\n",
        "\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        return f1\n",
        "\n",
        "    @staticmethod\n",
        "    def anls_score(prediction: str, ground_truth: str, threshold: float = 0.5) -> float:\n",
        "        \"\"\"计算ANLS分数\"\"\"\n",
        "        def levenshtein_distance(s1: str, s2: str) -> int:\n",
        "            if len(s1) < len(s2):\n",
        "                return levenshtein_distance(s2, s1)\n",
        "\n",
        "            if len(s2) == 0:\n",
        "                return len(s1)\n",
        "\n",
        "            previous_row = list(range(len(s2) + 1))\n",
        "            for i, c1 in enumerate(s1):\n",
        "                current_row = [i + 1]\n",
        "                for j, c2 in enumerate(s2):\n",
        "                    insertions = previous_row[j + 1] + 1\n",
        "                    deletions = current_row[j] + 1\n",
        "                    substitutions = previous_row[j] + (c1 != c2)\n",
        "                    current_row.append(min(insertions, deletions, substitutions))\n",
        "                previous_row = current_row\n",
        "\n",
        "            return previous_row[-1]\n",
        "\n",
        "        pred_norm = EvaluationMetrics.normalize_answer(prediction)\n",
        "        truth_norm = EvaluationMetrics.normalize_answer(ground_truth)\n",
        "\n",
        "        if len(truth_norm) == 0:\n",
        "            return 1.0 if len(pred_norm) == 0 else 0.0\n",
        "\n",
        "        edit_distance = levenshtein_distance(pred_norm, truth_norm)\n",
        "        max_len = max(len(pred_norm), len(truth_norm))\n",
        "\n",
        "        if max_len == 0:\n",
        "            return 1.0\n",
        "\n",
        "        normalized_distance = edit_distance / max_len\n",
        "        similarity = 1 - normalized_distance\n",
        "\n",
        "        return similarity if similarity >= threshold else 0.0\n",
        "\n",
        "    @staticmethod\n",
        "    def reranker_score(predictions: List[str], ground_truths: List[str], k: int = 1) -> float:\n",
        "        \"\"\"计算Reranker分数\"\"\"\n",
        "        if not predictions or not ground_truths:\n",
        "            return 0.0\n",
        "\n",
        "        top_k_preds = predictions[:k] if len(predictions) >= k else predictions\n",
        "\n",
        "        for pred in top_k_preds:\n",
        "            for truth in ground_truths:\n",
        "                if EvaluationMetrics.exact_match(pred, truth):\n",
        "                    return 1.0\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "def load_data(input_file: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"加载数据文件\"\"\"\n",
        "    data = []\n",
        "\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"文件不存在: {input_file}\")\n",
        "        return data\n",
        "\n",
        "    if input_file.endswith('.json'):\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "    elif input_file.endswith('.jsonl'):\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    data.append(json.loads(line.strip()))\n",
        "    else:\n",
        "        if os.path.isdir(input_file):\n",
        "            for filename in os.listdir(input_file):\n",
        "                if filename.endswith(('.json', '.jsonl')):\n",
        "                    filepath = os.path.join(input_file, filename)\n",
        "                    if filename.endswith('.json'):\n",
        "                        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                            file_data = json.load(f)\n",
        "                            if isinstance(file_data, list):\n",
        "                                data.extend(file_data)\n",
        "                            else:\n",
        "                                data.append(file_data)\n",
        "                    else:\n",
        "                        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                            for line in f:\n",
        "                                if line.strip():\n",
        "                                    data.append(json.loads(line.strip()))\n",
        "\n",
        "    return data\n",
        "\n",
        "def evaluate_model(input_file: str, output_file: str = None) -> Dict[str, float]:\n",
        "    \"\"\"评估模型性能\"\"\"\n",
        "    print(f\"正在加载数据: {input_file}\")\n",
        "    data = load_data(input_file)\n",
        "\n",
        "    if not data:\n",
        "        print(\"未能加载任何数据\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"加载了 {len(data)} 条数据\")\n",
        "\n",
        "    # 检查数据格式\n",
        "    print(\"\\n=== 数据格式检查 ===\")\n",
        "    sample_item = data[0]\n",
        "    if isinstance(sample_item, dict):\n",
        "        print(\"第一个样本的字段:\")\n",
        "        for key in sample_item.keys():\n",
        "            value = sample_item[key]\n",
        "            if isinstance(value, str):\n",
        "                preview = value[:50] + \"...\" if len(value) > 50 else value\n",
        "                print(f\"  {key}: '{preview}'\")\n",
        "\n",
        "    # 提取预测结果和真实标签\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "\n",
        "    for item in data:\n",
        "        if isinstance(item, dict):\n",
        "            # 根据您的数据格式，使用 predict 和 label 字段\n",
        "            if 'predict' in item and 'label' in item:\n",
        "                predictions.append(str(item['predict']))\n",
        "                ground_truths.append(str(item['label']))\n",
        "\n",
        "    if not predictions:\n",
        "        print(\"未能提取任何预测结果\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"成功提取 {len(predictions)} 条样本\")\n",
        "    print(f\"预测样例: {predictions[0][:100]}...\")\n",
        "    print(f\"真实标签样例: {ground_truths[0][:100]}...\")\n",
        "\n",
        "    print(f\"\\n正在计算评估指标...\")\n",
        "\n",
        "    # 计算各种指标\n",
        "    em_scores = []\n",
        "    f1_scores = []\n",
        "    anls_scores = []\n",
        "    reranker_scores = []\n",
        "\n",
        "    for pred, truth in zip(predictions, ground_truths):\n",
        "        em_scores.append(EvaluationMetrics.exact_match(pred, truth))\n",
        "        f1_scores.append(EvaluationMetrics.f1_score(pred, truth))\n",
        "        anls_scores.append(EvaluationMetrics.anls_score(pred, truth))\n",
        "        reranker_scores.append(EvaluationMetrics.reranker_score([pred], [truth]))\n",
        "\n",
        "    # 计算平均分数\n",
        "    results = {\n",
        "        'EM': np.mean(em_scores),\n",
        "        'F1': np.mean(f1_scores),\n",
        "        'ANLS': np.mean(anls_scores),\n",
        "        'Reranker': np.mean(reranker_scores),\n",
        "        'sample_count': len(predictions)\n",
        "    }\n",
        "\n",
        "    # 打印结果\n",
        "    print(\"\\n=== 评估结果 ===\")\n",
        "    print(f\"样本数量: {results['sample_count']}\")\n",
        "    print(f\"EM Score: {results['EM']:.4f} ({results['EM']*100:.2f}%)\")\n",
        "    print(f\"F1 Score: {results['F1']:.4f} ({results['F1']*100:.2f}%)\")\n",
        "    print(f\"ANLS Score: {results['ANLS']:.4f} ({results['ANLS']*100:.2f}%)\")\n",
        "    print(f\"Reranker Score: {results['Reranker']:.4f} ({results['Reranker']*100:.2f}%)\")\n",
        "\n",
        "    # 保存结果\n",
        "    if output_file:\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"\\n结果已保存到: {output_file}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# 直接运行评估\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/sharegpttest/generated_predictions.jsonl\"\n",
        "    results = evaluate_model(input_file, \"evaluation_results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX0UvkhrgiZT",
        "outputId": "98a4cdfe-66e8-4778-a483-d379777a3458"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在加载数据...\n",
            "加载了 548 条数据\n",
            "正在清理预测结果...\n",
            "\n",
            "--- 样本 1 清理对比 ---\n",
            "原始: Tony\n",
            "Tonyassistant\n",
            "Tony Robbins, in his book \"The 6 Human Needs,\" explains the 6 human needs that are at the core of our behavior and motivation. Thes...\n",
            "清理后: Tony Robbins, in his book \"The 6 Human Needs,\" explains the 6 human needs that are at the core of our behavior and motivation. These needs are: 1. Nee...\n",
            "\n",
            "--- 样本 2 清理对比 ---\n",
            "原始: Here are the 3 key questions you should be asking yourself when you are deciding which of the two approaches to using customer segmentation is right f...\n",
            "清理后: Here are the 3 key questions you should be asking yourself when you are deciding which of the two approaches to using customer segmentation is right f...\n",
            "\n",
            "--- 样本 3 清理对比 ---\n",
            "原始: ToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToTo...\n",
            "清理后: ToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToTo...\n",
            "\n",
            "--- 样本 4 清理对比 ---\n",
            "原始: By the grace of the gods, the arcane and enigmatic art of metaphorical language has been summoned forth to illuminate the enigmatic addressing modes o...\n",
            "清理后: By the grace of the gods, arcane and enigmatic art of metaphorical language has been summoned forth to illuminate enigmatic addressing modes instructi...\n",
            "\n",
            "--- 样本 5 清理对比 ---\n",
            "原始: In the function `add\\_player(vector& players)`, the `getline()` function is used to read the name of the new player from the user. The `getline()` fun...\n",
            "清理后: In the function `add\\_player(vector& players)`, the `getline()` function is used to read name of new player from. `getline()` reads name of new player...\n",
            "\n",
            "改进了 533 个样本 (97.3%)\n",
            "\n",
            "正在重新计算评估指标...\n",
            "\n",
            "=== 清理前后对比 ===\n",
            "样本数量: 548\n",
            "\n",
            "指标              原始         清理后        改进        \n",
            "--------------------------------------------------\n",
            "EM              0.0000     0.0000     +0.0000\n",
            "F1              0.2642     0.2421     -0.0221\n",
            "Reranker        0.1131     0.0529     -0.0602\n",
            "\n",
            "=== 改进最明显的样本 ===\n",
            "\n",
            "样本 131 (改进 +0.246):\n",
            "原始: SureSure\n",
            "I think this might be a great case study for you. I think this is a great case study for yo...\n",
            "清理: Sure I think this might be a great case study for you. I think this is a great case study for you. m...\n",
            "标签: Yes, there are many success stories of used car dealers using Unbounce to increase conversions and d...\n",
            "\n",
            "样本 535 (改进 +0.222):\n",
            "原始: In that to B company?? yourFRSS 13 fair value of? it? please me please. IFRS this. fair value? ifSur...\n",
            "清理: In that to B company. yourFRSS 13 fair value of. it. please me please. IFRS this. fair value. ifSure...\n",
            "标签: Whether a transaction between two companies is considered fair based on IFRS 13 fair value terms dep...\n",
            "\n",
            "样本 268 (改进 +0.201):\n",
            "原始: Sure\n",
            "Sureassistant\n",
            "Sure! Here's a table roadmap for the first half of 2023 to continue to secure key...\n",
            "清理: Sure Sure. Here's a table roadmap for the first half of 2023 to continue to secure key partners in t...\n",
            "标签: Sure, here's a roadmap for the 'plan' to continue securing key partners in the app store (including ...\n",
            "\n",
            "正在保存清理后的数据到: /content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/cleaned_predictions.jsonl\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "class PredictionCleaner:\n",
        "    \"\"\"用于清理和改进预测结果的类\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_repetitions(text: str, max_repeat: int = 3) -> str:\n",
        "        \"\"\"移除过度重复的词汇\"\"\"\n",
        "        words = text.split()\n",
        "        if not words:\n",
        "            return text\n",
        "\n",
        "        cleaned_words = []\n",
        "        word_count = Counter()\n",
        "\n",
        "        for word in words:\n",
        "            word_lower = word.lower().strip('.,!?;:')\n",
        "            word_count[word_lower] += 1\n",
        "\n",
        "            # 如果这个词已经重复太多次，跳过\n",
        "            if word_count[word_lower] <= max_repeat:\n",
        "                cleaned_words.append(word)\n",
        "            elif len(cleaned_words) == 0:  # 保留至少一个词\n",
        "                cleaned_words.append(word)\n",
        "\n",
        "        return ' '.join(cleaned_words)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_format_tokens(text: str) -> str:\n",
        "        \"\"\"移除格式标记和错误的系统词汇\"\"\"\n",
        "        # 移除chat格式标记\n",
        "        text = re.sub(r'<\\|im_start\\|>.*?<\\|im_end\\|>', '', text, flags=re.DOTALL)\n",
        "        text = re.sub(r'<\\|.*?\\|>', '', text)\n",
        "\n",
        "        # 移除assistant相关词汇\n",
        "        text = re.sub(r'\\b(assistant|system|user)\\b', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 移除开头的重复名字（如 \"Tony Tonyassistant\"）\n",
        "        text = re.sub(r'^(\\w+)\\s*\\1\\s*(assistant|system)?\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 移除其他常见错误格式\n",
        "        text = re.sub(r'\\bhelpful assistant\\b', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_sentence_repetitions(text: str) -> str:\n",
        "        \"\"\"移除重复的句子\"\"\"\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        seen_sentences = set()\n",
        "        unique_sentences = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_clean = sentence.strip().lower()\n",
        "            if sentence_clean and sentence_clean not in seen_sentences:\n",
        "                seen_sentences.add(sentence_clean)\n",
        "                unique_sentences.append(sentence.strip())\n",
        "\n",
        "        return '. '.join(s for s in unique_sentences if s) + ('.' if unique_sentences else '')\n",
        "\n",
        "    @staticmethod\n",
        "    def fix_common_errors(text: str) -> str:\n",
        "        \"\"\"修复常见错误\"\"\"\n",
        "        # 修复粘连的词（如 \"Tonyassistant\" -> \"Tony\"）\n",
        "        text = re.sub(r'(\\w+)assistant', r'\\1', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'(\\w+)system', r'\\1', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 修复多余的换行和空格\n",
        "        text = re.sub(r'\\n+', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # 修复开头的错误（移除开头重复的词）\n",
        "        words = text.split()\n",
        "        if len(words) > 1 and words[0].lower() == words[1].lower():\n",
        "            text = ' '.join(words[1:])\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    @classmethod\n",
        "    def clean_prediction(cls, text: str) -> str:\n",
        "        \"\"\"综合清理预测文本\"\"\"\n",
        "        # 按步骤清理\n",
        "        text = cls.remove_format_tokens(text)\n",
        "        text = cls.fix_common_errors(text)\n",
        "        text = cls.remove_repetitions(text, max_repeat=2)  # 更严格的重复控制\n",
        "        text = cls.remove_sentence_repetitions(text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "def clean_and_evaluate(input_file: str, output_file: str = None):\n",
        "    \"\"\"清理预测并重新评估\"\"\"\n",
        "\n",
        "    # 加载数据\n",
        "    print(\"正在加载数据...\")\n",
        "    data = []\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line.strip()))\n",
        "\n",
        "    print(f\"加载了 {len(data)} 条数据\")\n",
        "\n",
        "    # 清理预测\n",
        "    print(\"正在清理预测结果...\")\n",
        "    cleaned_data = []\n",
        "    improvement_count = 0\n",
        "\n",
        "    for i, item in enumerate(data):\n",
        "        original_pred = item['predict']\n",
        "        cleaned_pred = PredictionCleaner.clean_prediction(original_pred)\n",
        "\n",
        "        # 检查是否有改进\n",
        "        if cleaned_pred != original_pred:\n",
        "            improvement_count += 1\n",
        "\n",
        "        cleaned_item = item.copy()\n",
        "        cleaned_item['original_predict'] = original_pred\n",
        "        cleaned_item['predict'] = cleaned_pred\n",
        "        cleaned_data.append(cleaned_item)\n",
        "\n",
        "        # 显示前几个清理示例\n",
        "        if i < 5:\n",
        "            print(f\"\\n--- 样本 {i+1} 清理对比 ---\")\n",
        "            print(f\"原始: {original_pred[:150]}...\")\n",
        "            print(f\"清理后: {cleaned_pred[:150]}...\")\n",
        "\n",
        "    print(f\"\\n改进了 {improvement_count} 个样本 ({improvement_count/len(data)*100:.1f}%)\")\n",
        "\n",
        "    # 重新计算评估指标\n",
        "    print(\"\\n正在重新计算评估指标...\")\n",
        "\n",
        "    # 评估函数\n",
        "    def normalize_answer(s: str) -> str:\n",
        "        def remove_articles(text):\n",
        "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "        def white_space_fix(text):\n",
        "            return ' '.join(text.split())\n",
        "        def remove_punc(text):\n",
        "            exclude = set(string.punctuation)\n",
        "            return ''.join(ch for ch in text if ch not in exclude)\n",
        "        def lower(text):\n",
        "            return text.lower()\n",
        "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "    def exact_match(prediction: str, ground_truth: str) -> float:\n",
        "        return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "    def f1_score(prediction: str, ground_truth: str) -> float:\n",
        "        pred_tokens = normalize_answer(prediction).split()\n",
        "        truth_tokens = normalize_answer(ground_truth).split()\n",
        "\n",
        "        if len(pred_tokens) == 0 and len(truth_tokens) == 0:\n",
        "            return 1.0\n",
        "        if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        common = Counter(pred_tokens) & Counter(truth_tokens)\n",
        "        num_same = sum(common.values())\n",
        "\n",
        "        precision = num_same / len(pred_tokens)\n",
        "        recall = num_same / len(truth_tokens)\n",
        "\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "\n",
        "        return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    def partial_match_score(prediction: str, ground_truth: str, threshold: float = 0.7) -> float:\n",
        "        \"\"\"计算部分匹配分数，用于改进的Reranker\"\"\"\n",
        "        f1 = f1_score(prediction, ground_truth)\n",
        "        return 1.0 if f1 >= threshold else 0.0\n",
        "\n",
        "    # 计算原始分数和清理后分数\n",
        "    original_em_scores = []\n",
        "    original_f1_scores = []\n",
        "    original_reranker_scores = []\n",
        "\n",
        "    cleaned_em_scores = []\n",
        "    cleaned_f1_scores = []\n",
        "    cleaned_reranker_scores = []\n",
        "\n",
        "    for item in cleaned_data:\n",
        "        original_pred = item['original_predict']\n",
        "        cleaned_pred = item['predict']\n",
        "        label = item['label']\n",
        "\n",
        "        # 原始分数\n",
        "        original_em_scores.append(exact_match(original_pred, label))\n",
        "        original_f1_scores.append(f1_score(original_pred, label))\n",
        "        original_reranker_scores.append(partial_match_score(original_pred, label, 0.5))\n",
        "\n",
        "        # 清理后分数\n",
        "        cleaned_em_scores.append(exact_match(cleaned_pred, label))\n",
        "        cleaned_f1_scores.append(f1_score(cleaned_pred, label))\n",
        "        cleaned_reranker_scores.append(partial_match_score(cleaned_pred, label, 0.5))\n",
        "\n",
        "    # 计算平均分数\n",
        "    original_results = {\n",
        "        'EM': np.mean(original_em_scores),\n",
        "        'F1': np.mean(original_f1_scores),\n",
        "        'Reranker': np.mean(original_reranker_scores)\n",
        "    }\n",
        "\n",
        "    cleaned_results = {\n",
        "        'EM': np.mean(cleaned_em_scores),\n",
        "        'F1': np.mean(cleaned_f1_scores),\n",
        "        'Reranker': np.mean(cleaned_reranker_scores)\n",
        "    }\n",
        "\n",
        "    # 打印对比结果\n",
        "    print(\"\\n=== 清理前后对比 ===\")\n",
        "    print(f\"样本数量: {len(data)}\")\n",
        "    print(f\"\\n{'指标':<15} {'原始':<10} {'清理后':<10} {'改进':<10}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for metric in ['EM', 'F1', 'Reranker']:\n",
        "        original = original_results[metric]\n",
        "        cleaned = cleaned_results[metric]\n",
        "        improvement = cleaned - original\n",
        "        print(f\"{metric:<15} {original:<10.4f} {cleaned:<10.4f} {improvement:>+.4f}\")\n",
        "\n",
        "    # 找出改进最明显的样本\n",
        "    print(\"\\n=== 改进最明显的样本 ===\")\n",
        "    improvements = []\n",
        "    for i, item in enumerate(cleaned_data):\n",
        "        original_f1 = f1_score(item['original_predict'], item['label'])\n",
        "        cleaned_f1 = f1_score(item['predict'], item['label'])\n",
        "        improvement = cleaned_f1 - original_f1\n",
        "        if improvement > 0.1:  # 改进超过0.1的样本\n",
        "            improvements.append((i, improvement, item))\n",
        "\n",
        "    improvements.sort(key=lambda x: x[1], reverse=True)\n",
        "    for i, (idx, improvement, item) in enumerate(improvements[:3]):\n",
        "        print(f\"\\n样本 {idx} (改进 +{improvement:.3f}):\")\n",
        "        print(f\"原始: {item['original_predict'][:100]}...\")\n",
        "        print(f\"清理: {item['predict'][:100]}...\")\n",
        "        print(f\"标签: {item['label'][:100]}...\")\n",
        "\n",
        "    # 保存清理后的数据\n",
        "    if output_file:\n",
        "        print(f\"\\n正在保存清理后的数据到: {output_file}\")\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            for item in cleaned_data:\n",
        "                # 只保存清理后的predict，移除original_predict以节省空间\n",
        "                save_item = {k: v for k, v in item.items() if k != 'original_predict'}\n",
        "                f.write(json.dumps(save_item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    return original_results, cleaned_results\n",
        "\n",
        "# 运行清理和评估\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/sharegpttest/generated_predictions.jsonl\"\n",
        "    output_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/cleaned_predictions.jsonl\"\n",
        "\n",
        "    original_results, cleaned_results = clean_and_evaluate(input_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yXevK0yhGsp",
        "outputId": "3a916b2d-7a3f-42e5-edd7-27145bec2a15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在加载数据...\n",
            "加载了 548 条数据\n",
            "正在分析和清理预测...\n",
            "\n",
            "清理策略统计:\n",
            "  conservative: 528 个样本 (96.4%)\n",
            "  garbage_kept: 7 个样本 (1.3%)\n",
            "  garbage_reconstruction: 9 个样本 (1.6%)\n",
            "  repetition_fix: 4 个样本 (0.7%)\n",
            "\n",
            "=== 发现 16 个严重问题样本 ===\n",
            "\n",
            "样本 2 (garbage_kept):\n",
            "原始: ToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToTo...\n",
            "清理: ToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToTo...\n",
            "标签: You can use the `String.format()` method in Java to replace placeholders in a string with values from a map. Here's an example code snippet that demonstrates how you can achieve this:\n",
            "```java\n",
            "import j...\n",
            "\n",
            "样本 27 (garbage_reconstruction):\n",
            "原始: Sure, I can help you implement a netfilter in Linux using the Linux kernel module. Here's an example of how you can do it using Rust:\n",
            "First, you need to install the `netfilter-iptables` module in your...\n",
            "清理: Sure, I can help you implement a netfilter in Linux using the Linux kernel module. Here's an example of how\n",
            "标签: Implementing a netfilter in Linux using Rust requires a deep understanding of both the Linux kernel and Rust programming language. Here is a high-level overview of the steps involved:\n",
            "\n",
            "1. Install Rust...\n",
            "\n",
            "样本 121 (garbage_kept):\n",
            "原始: ToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToTo...\n",
            "清理: ToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToToTo...\n",
            "标签: To make an HTTP request in JavaScript, you can use the `XMLHttpRequest` object or the more modern `fetch()` function.\n",
            "\n",
            "Here's an example of making an HTTP GET request using `XMLHttpRequest`:\n",
            "```\n",
            "var x...\n",
            "\n",
            "正在重新计算评估指标...\n",
            "\n",
            "=== 智能清理结果 ===\n",
            "总体改进:\n",
            "  原始F1: 0.2642\n",
            "  清理后F1: 0.2617 (改进: -0.0025)\n",
            "  EM: 0.0000\n",
            "  智能Reranker: 0.2810\n",
            "\n",
            "按策略分类的结果:\n",
            "  conservative (528 样本):\n",
            "    EM: 0.0000, F1: 0.2696, Reranker: 0.2898\n",
            "  repetition_fix (4 样本):\n",
            "    EM: 0.0000, F1: 0.1037, Reranker: 0.2500\n",
            "  garbage_reconstruction (9 样本):\n",
            "    EM: 0.0000, F1: 0.0740, Reranker: 0.0000\n",
            "  garbage_kept (7 样本):\n",
            "    EM: 0.0000, F1: 0.0000, Reranker: 0.0000\n",
            "\n",
            "正在保存清理后的数据到: /content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/smart_cleaned_predictions.jsonl\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "class SmartCleaner:\n",
        "    \"\"\"更智能的预测清理器\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_severe_repetition(text: str) -> bool:\n",
        "        \"\"\"检测严重的重复问题\"\"\"\n",
        "        words = text.split()\n",
        "        if len(words) < 5:\n",
        "            return False\n",
        "\n",
        "        # 检查是否有词汇重复超过总词数的50%\n",
        "        word_counts = Counter(words)\n",
        "        max_count = max(word_counts.values()) if word_counts else 0\n",
        "        repetition_ratio = max_count / len(words)\n",
        "\n",
        "        return repetition_ratio > 0.5\n",
        "\n",
        "    @staticmethod\n",
        "    def is_garbage_output(text: str) -> bool:\n",
        "        \"\"\"判断是否为垃圾输出\"\"\"\n",
        "        # 检查各种垃圾输出模式\n",
        "        text_lower = text.lower().strip()\n",
        "\n",
        "        # 完全重复的单词\n",
        "        words = text.split()\n",
        "        if len(words) > 10:\n",
        "            unique_words = set(words)\n",
        "            if len(unique_words) <= 3:  # 只有很少的不同词汇\n",
        "                return True\n",
        "\n",
        "        # 检查是否主要是重复字符\n",
        "        if len(text) > 50:\n",
        "            char_counts = Counter(text.replace(' ', '').replace('\\n', ''))\n",
        "            if char_counts and max(char_counts.values()) > len(text) * 0.4:\n",
        "                return True\n",
        "\n",
        "        # 检查是否包含明显的格式错误\n",
        "        error_patterns = [\n",
        "            r'^(\\w{1,4})\\1{10,}',  # 短词重复很多次\n",
        "            r'(assistant|system|user)\\s*\\1',  # 系统词重复\n",
        "        ]\n",
        "\n",
        "        for pattern in error_patterns:\n",
        "            if re.search(pattern, text_lower):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def smart_repetition_fix(text: str) -> str:\n",
        "        \"\"\"智能修复重复问题\"\"\"\n",
        "        # 修复开头的重复问题\n",
        "        text = re.sub(r'^(\\w+)\\s*\\1\\s*(assistant|system)?\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 修复句子级别的重复\n",
        "        sentences = re.split(r'([.!?]+)', text)\n",
        "        cleaned_sentences = []\n",
        "        seen_sentences = set()\n",
        "\n",
        "        for i in range(0, len(sentences), 2):\n",
        "            if i < len(sentences):\n",
        "                sentence = sentences[i].strip()\n",
        "                sentence_key = re.sub(r'\\s+', ' ', sentence.lower())\n",
        "\n",
        "                if sentence_key and sentence_key not in seen_sentences and len(sentence) > 10:\n",
        "                    seen_sentences.add(sentence_key)\n",
        "                    cleaned_sentences.append(sentence)\n",
        "                    if i + 1 < len(sentences):\n",
        "                        cleaned_sentences.append(sentences[i + 1])  # 保留标点\n",
        "\n",
        "        result = ''.join(cleaned_sentences).strip()\n",
        "\n",
        "        # 如果清理后太短，使用原文\n",
        "        if len(result) < len(text) * 0.3:\n",
        "            return text\n",
        "\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def conservative_clean(text: str) -> str:\n",
        "        \"\"\"保守的清理策略\"\"\"\n",
        "        original_text = text\n",
        "\n",
        "        # 只修复明显的问题\n",
        "        # 1. 移除格式标记\n",
        "        text = re.sub(r'<\\|im_start\\|>.*?<\\|im_end\\|>', '', text, flags=re.DOTALL)\n",
        "\n",
        "        # 2. 修复开头的重复词和格式错误\n",
        "        text = re.sub(r'^(\\w+)\\s*\\1\\s*assistant\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'^(\\w+)\\s*assistant\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 3. 移除standalone的assistant/system/user\n",
        "        text = re.sub(r'\\b(assistant|system|user)\\s+', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 4. 修复多余空格\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # 如果清理后明显变差，返回原文\n",
        "        if len(text) < len(original_text) * 0.7:\n",
        "            return original_text\n",
        "\n",
        "        return text\n",
        "\n",
        "    @classmethod\n",
        "    def process_prediction(cls, text: str) -> Tuple[str, str]:\n",
        "        \"\"\"\n",
        "        处理预测文本\n",
        "        返回: (清理后的文本, 处理策略)\n",
        "        \"\"\"\n",
        "        # 检查是否为严重的垃圾输出\n",
        "        if cls.is_garbage_output(text):\n",
        "            # 对于垃圾输出，尝试从中提取有意义的部分\n",
        "            words = text.split()\n",
        "            if words:\n",
        "                # 尝试找到第一个正常的词开始\n",
        "                meaningful_start = 0\n",
        "                for i, word in enumerate(words):\n",
        "                    if len(word) > 2 and word.lower() not in ['to', 'the', 'and', 'or', 'but']:\n",
        "                        meaningful_start = i\n",
        "                        break\n",
        "\n",
        "                # 取前面一些正常词汇\n",
        "                if meaningful_start < len(words) - 5:\n",
        "                    reconstructed = ' '.join(words[meaningful_start:meaningful_start + 20])\n",
        "                    return reconstructed, \"garbage_reconstruction\"\n",
        "\n",
        "            return text, \"garbage_kept\"\n",
        "\n",
        "        # 检查严重重复\n",
        "        elif cls.detect_severe_repetition(text):\n",
        "            cleaned = cls.smart_repetition_fix(text)\n",
        "            return cleaned, \"repetition_fix\"\n",
        "\n",
        "        # 保守清理\n",
        "        else:\n",
        "            cleaned = cls.conservative_clean(text)\n",
        "            return cleaned, \"conservative\"\n",
        "\n",
        "def advanced_evaluation(input_file: str, output_file: str = None):\n",
        "    \"\"\"高级评估和清理\"\"\"\n",
        "\n",
        "    # 加载数据\n",
        "    print(\"正在加载数据...\")\n",
        "    data = []\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line.strip()))\n",
        "\n",
        "    print(f\"加载了 {len(data)} 条数据\")\n",
        "\n",
        "    # 分析和清理\n",
        "    print(\"正在分析和清理预测...\")\n",
        "\n",
        "    strategy_counts = Counter()\n",
        "    cleaned_data = []\n",
        "    severe_issues = []\n",
        "\n",
        "    for i, item in enumerate(data):\n",
        "        original_pred = item['predict']\n",
        "        cleaned_pred, strategy = SmartCleaner.process_prediction(original_pred)\n",
        "\n",
        "        strategy_counts[strategy] += 1\n",
        "\n",
        "        # 记录严重问题的样本\n",
        "        if strategy in [\"garbage_reconstruction\", \"garbage_kept\"]:\n",
        "            severe_issues.append({\n",
        "                'index': i,\n",
        "                'strategy': strategy,\n",
        "                'original': original_pred[:200] + \"...\" if len(original_pred) > 200 else original_pred,\n",
        "                'cleaned': cleaned_pred[:200] + \"...\" if len(cleaned_pred) > 200 else cleaned_pred,\n",
        "                'label': item['label'][:200] + \"...\" if len(item['label']) > 200 else item['label']\n",
        "            })\n",
        "\n",
        "        item_copy = item.copy()\n",
        "        item_copy['original_predict'] = original_pred\n",
        "        item_copy['predict'] = cleaned_pred\n",
        "        item_copy['clean_strategy'] = strategy\n",
        "        cleaned_data.append(item_copy)\n",
        "\n",
        "    print(f\"\\n清理策略统计:\")\n",
        "    for strategy, count in strategy_counts.items():\n",
        "        print(f\"  {strategy}: {count} 个样本 ({count/len(data)*100:.1f}%)\")\n",
        "\n",
        "    # 显示严重问题样本\n",
        "    if severe_issues:\n",
        "        print(f\"\\n=== 发现 {len(severe_issues)} 个严重问题样本 ===\")\n",
        "        for i, issue in enumerate(severe_issues[:3]):\n",
        "            print(f\"\\n样本 {issue['index']} ({issue['strategy']}):\")\n",
        "            print(f\"原始: {issue['original']}\")\n",
        "            print(f\"清理: {issue['cleaned']}\")\n",
        "            print(f\"标签: {issue['label']}\")\n",
        "\n",
        "    # 重新评估\n",
        "    print(\"\\n正在重新计算评估指标...\")\n",
        "\n",
        "    def normalize_answer(s: str) -> str:\n",
        "        def remove_articles(text):\n",
        "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "        def white_space_fix(text):\n",
        "            return ' '.join(text.split())\n",
        "        def remove_punc(text):\n",
        "            exclude = set(string.punctuation)\n",
        "            return ''.join(ch for ch in text if ch not in exclude)\n",
        "        def lower(text):\n",
        "            return text.lower()\n",
        "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "    def exact_match(prediction: str, ground_truth: str) -> float:\n",
        "        return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "    def f1_score(prediction: str, ground_truth: str) -> float:\n",
        "        pred_tokens = normalize_answer(prediction).split()\n",
        "        truth_tokens = normalize_answer(ground_truth).split()\n",
        "\n",
        "        if len(pred_tokens) == 0 and len(truth_tokens) == 0:\n",
        "            return 1.0\n",
        "        if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        common = Counter(pred_tokens) & Counter(truth_tokens)\n",
        "        num_same = sum(common.values())\n",
        "\n",
        "        if num_same == 0:\n",
        "            return 0.0\n",
        "\n",
        "        precision = num_same / len(pred_tokens)\n",
        "        recall = num_same / len(truth_tokens)\n",
        "\n",
        "        return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    def smart_reranker(prediction: str, ground_truth: str) -> float:\n",
        "        \"\"\"智能Reranker，使用多个标准\"\"\"\n",
        "        # 标准1: F1分数高于阈值\n",
        "        f1 = f1_score(prediction, ground_truth)\n",
        "        if f1 >= 0.6:\n",
        "            return 1.0\n",
        "\n",
        "        # 标准2: 关键词匹配\n",
        "        pred_words = set(normalize_answer(prediction).split())\n",
        "        truth_words = set(normalize_answer(ground_truth).split())\n",
        "\n",
        "        if len(truth_words) > 0:\n",
        "            keyword_overlap = len(pred_words & truth_words) / len(truth_words)\n",
        "            if keyword_overlap >= 0.4:\n",
        "                return 1.0\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    # 计算分数\n",
        "    strategies = ['conservative', 'repetition_fix', 'garbage_reconstruction', 'garbage_kept']\n",
        "    results_by_strategy = {}\n",
        "\n",
        "    for strategy in strategies:\n",
        "        strategy_items = [item for item in cleaned_data if item['clean_strategy'] == strategy]\n",
        "        if not strategy_items:\n",
        "            continue\n",
        "\n",
        "        em_scores = []\n",
        "        f1_scores = []\n",
        "        reranker_scores = []\n",
        "\n",
        "        for item in strategy_items:\n",
        "            pred = item['predict']\n",
        "            label = item['label']\n",
        "\n",
        "            em_scores.append(exact_match(pred, label))\n",
        "            f1_scores.append(f1_score(pred, label))\n",
        "            reranker_scores.append(smart_reranker(pred, label))\n",
        "\n",
        "        results_by_strategy[strategy] = {\n",
        "            'count': len(strategy_items),\n",
        "            'EM': np.mean(em_scores),\n",
        "            'F1': np.mean(f1_scores),\n",
        "            'Reranker': np.mean(reranker_scores)\n",
        "        }\n",
        "\n",
        "    # 计算总体分数\n",
        "    all_em_scores = []\n",
        "    all_f1_scores = []\n",
        "    all_reranker_scores = []\n",
        "    all_original_f1_scores = []\n",
        "\n",
        "    for item in cleaned_data:\n",
        "        pred = item['predict']\n",
        "        orig_pred = item['original_predict']\n",
        "        label = item['label']\n",
        "\n",
        "        all_em_scores.append(exact_match(pred, label))\n",
        "        all_f1_scores.append(f1_score(pred, label))\n",
        "        all_reranker_scores.append(smart_reranker(pred, label))\n",
        "        all_original_f1_scores.append(f1_score(orig_pred, label))\n",
        "\n",
        "    overall_results = {\n",
        "        'EM': np.mean(all_em_scores),\n",
        "        'F1': np.mean(all_f1_scores),\n",
        "        'Reranker': np.mean(all_reranker_scores)\n",
        "    }\n",
        "\n",
        "    original_f1 = np.mean(all_original_f1_scores)\n",
        "\n",
        "    # 打印结果\n",
        "    print(f\"\\n=== 智能清理结果 ===\")\n",
        "    print(f\"总体改进:\")\n",
        "    print(f\"  原始F1: {original_f1:.4f}\")\n",
        "    print(f\"  清理后F1: {overall_results['F1']:.4f} (改进: {overall_results['F1'] - original_f1:+.4f})\")\n",
        "    print(f\"  EM: {overall_results['EM']:.4f}\")\n",
        "    print(f\"  智能Reranker: {overall_results['Reranker']:.4f}\")\n",
        "\n",
        "    print(f\"\\n按策略分类的结果:\")\n",
        "    for strategy, results in results_by_strategy.items():\n",
        "        print(f\"  {strategy} ({results['count']} 样本):\")\n",
        "        print(f\"    EM: {results['EM']:.4f}, F1: {results['F1']:.4f}, Reranker: {results['Reranker']:.4f}\")\n",
        "\n",
        "    # 保存结果\n",
        "    if output_file:\n",
        "        print(f\"\\n正在保存清理后的数据到: {output_file}\")\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            for item in cleaned_data:\n",
        "                # 保存清理后的数据，包含策略信息\n",
        "                save_item = {\n",
        "                    'prompt': item['prompt'],\n",
        "                    'predict': item['predict'],\n",
        "                    'label': item['label'],\n",
        "                    'clean_strategy': item['clean_strategy']\n",
        "                }\n",
        "                f.write(json.dumps(save_item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    return overall_results, results_by_strategy\n",
        "\n",
        "# 运行智能清理\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/sharegpttest/generated_predictions.jsonl\"\n",
        "    output_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/smart_cleaned_predictions.jsonl\"\n",
        "\n",
        "    overall_results, strategy_results = advanced_evaluation(input_file, output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM4n36FyjN_e"
      },
      "source": [
        "lora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCeMCZwDkhMH",
        "outputId": "cf5145ec-b3fb-4744-9b6c-f00db43e19c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 失败模式分析 ===\n",
            "\n",
            "各策略的详细分析:\n",
            "\n",
            "🔍 垃圾输出分析 (19 样本):\n",
            "  other: 8 样本\n",
            "  Sure_repetition: 3 样本\n",
            "  extreme_repetition: 8 样本\n",
            "\n",
            "📊 Conservative策略分析 (526 样本):\n",
            "  低分样本 (F1<0.1): 135 (25.7%)\n",
            "  中等样本 (0.1≤F1<0.3): 228 (43.3%)\n",
            "  良好样本 (F1≥0.3): 163 (31.0%)\n",
            "\n",
            "🔍 低分样本问题分析:\n",
            "  too_long: 49 样本 (36.3%)\n",
            "  topic_mismatch: 103 样本 (76.3%)\n",
            "  too_short: 69 样本 (51.1%)\n",
            "  format_mismatch: 3 样本 (2.2%)\n",
            "\n",
            "🎯 === 改进建议 ===\n",
            "\n",
            "📝 短期改进（立即可行）:\n",
            "1. **优化后处理管道**:\n",
            "   - 当前Reranker已达到25%，表现不错\n",
            "   - 可以调整智能Reranker的阈值来平衡精确度和召回率\n",
            "   - 对96%的良好样本，可以进一步微调清理规则\n",
            "\n",
            "2. **处理垃圾样本**:\n",
            "   - 19 个垃圾样本可以考虑直接过滤或用规则生成\n",
            "   - 这些样本可能来自训练数据中的问题样本\n",
            "\n",
            "🔄 中期改进（重新训练）:\n",
            "1. **数据质量优化**:\n",
            "   - 清理训练数据中可能导致重复输出的样本\n",
            "   - 增加数据多样性，特别是针对低分样本的类似场景\n",
            "2. **训练参数调优**:\n",
            "   - 增加repetition_penalty (建议1.1-1.2)\n",
            "   - 调整temperature (建议0.7-0.9)\n",
            "   - 考虑使用top_p采样 (0.8-0.95)\n",
            "3. **训练策略改进**:\n",
            "   - 使用更好的停止条件\n",
            "   - 考虑添加更多轮次的训练\n",
            "   - 可以尝试DPO (Direct Preference Optimization)\n",
            "\n",
            "🚀 长期改进（架构升级）:\n",
            "1. 考虑使用更大的模型或更新的架构\n",
            "2. 实施RLHF (人类反馈强化学习)\n",
            "3. 使用多轮对话训练提升一致性\n",
            "\n",
            "📊 评估改进建议:\n",
            "1. **当前Reranker 25%已经很好**，可以考虑:\n",
            "   - 调整阈值获得更高分数\n",
            "   - 使用语义相似度 (如sentence-transformers)\n",
            "   - 实施多级评估标准\n",
            "\n",
            "2. **EM分数为0是正常的**，因为:\n",
            "   - 生成任务很少有完全匹配\n",
            "   - F1和Reranker分数更有意义\n",
            "   - 您的模型实际表现比EM显示的要好\n",
            "\n",
            "💎 创建高质量数据集:\n",
            "   原始样本: 548\n",
            "   高质量样本 (F1≥0.2): 274\n",
            "   保留率: 50.0%\n",
            "   已保存到: /content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/high_quality_samples.jsonl\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "def analyze_failure_patterns(input_file: str):\n",
        "    \"\"\"分析失败模式，给出针对性建议\"\"\"\n",
        "\n",
        "    # 加载清理后的数据\n",
        "    data = []\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line.strip()))\n",
        "\n",
        "    print(\"=== 失败模式分析 ===\")\n",
        "\n",
        "    # 按策略分组分析\n",
        "    strategy_groups = defaultdict(list)\n",
        "    for item in data:\n",
        "        strategy_groups[item['clean_strategy']].append(item)\n",
        "\n",
        "    print(f\"\\n各策略的详细分析:\")\n",
        "\n",
        "    # 分析垃圾输出的原因\n",
        "    garbage_samples = strategy_groups['garbage_reconstruction'] + strategy_groups['garbage_kept']\n",
        "\n",
        "    if garbage_samples:\n",
        "        print(f\"\\n🔍 垃圾输出分析 ({len(garbage_samples)} 样本):\")\n",
        "\n",
        "        garbage_patterns = Counter()\n",
        "        for item in garbage_samples:\n",
        "            pred = item['predict']\n",
        "\n",
        "            # 分析垃圾类型\n",
        "            if 'Sure' in pred and pred.count('Sure') > 5:\n",
        "                garbage_patterns['Sure_repetition'] += 1\n",
        "            elif len(set(pred.split())) <= 3:\n",
        "                garbage_patterns['extreme_repetition'] += 1\n",
        "            elif 'e-' in pred and pred.count('e-') > 3:\n",
        "                garbage_patterns['number_spam'] += 1\n",
        "            else:\n",
        "                garbage_patterns['other'] += 1\n",
        "\n",
        "        for pattern, count in garbage_patterns.items():\n",
        "            print(f\"  {pattern}: {count} 样本\")\n",
        "\n",
        "    # 分析conservative策略中的低分样本\n",
        "    conservative_samples = strategy_groups['conservative']\n",
        "\n",
        "    def calculate_f1(pred, label):\n",
        "        import re, string\n",
        "        from collections import Counter\n",
        "\n",
        "        def normalize(s):\n",
        "            s = re.sub(r'\\s+', ' ', s.lower())\n",
        "            s = ''.join(c for c in s if c not in string.punctuation)\n",
        "            return s.split()\n",
        "\n",
        "        pred_tokens = normalize(pred)\n",
        "        label_tokens = normalize(label)\n",
        "\n",
        "        if not pred_tokens and not label_tokens:\n",
        "            return 1.0\n",
        "        if not pred_tokens or not label_tokens:\n",
        "            return 0.0\n",
        "\n",
        "        common = Counter(pred_tokens) & Counter(label_tokens)\n",
        "        num_same = sum(common.values())\n",
        "\n",
        "        if num_same == 0:\n",
        "            return 0.0\n",
        "\n",
        "        precision = num_same / len(pred_tokens)\n",
        "        recall = num_same / len(label_tokens)\n",
        "        return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    # 计算每个样本的F1分数\n",
        "    conservative_f1_scores = []\n",
        "    for item in conservative_samples:\n",
        "        f1 = calculate_f1(item['predict'], item['label'])\n",
        "        conservative_f1_scores.append((f1, item))\n",
        "\n",
        "    conservative_f1_scores.sort(key=lambda x: x[0])\n",
        "\n",
        "    print(f\"\\n📊 Conservative策略分析 ({len(conservative_samples)} 样本):\")\n",
        "\n",
        "    low_f1_samples = [item for f1, item in conservative_f1_scores if f1 < 0.1]\n",
        "    medium_f1_samples = [item for f1, item in conservative_f1_scores if 0.1 <= f1 < 0.3]\n",
        "    good_f1_samples = [item for f1, item in conservative_f1_scores if f1 >= 0.3]\n",
        "\n",
        "    print(f\"  低分样本 (F1<0.1): {len(low_f1_samples)} ({len(low_f1_samples)/len(conservative_samples)*100:.1f}%)\")\n",
        "    print(f\"  中等样本 (0.1≤F1<0.3): {len(medium_f1_samples)} ({len(medium_f1_samples)/len(conservative_samples)*100:.1f}%)\")\n",
        "    print(f\"  良好样本 (F1≥0.3): {len(good_f1_samples)} ({len(good_f1_samples)/len(conservative_samples)*100:.1f}%)\")\n",
        "\n",
        "    # 分析低分样本的问题\n",
        "    if low_f1_samples:\n",
        "        print(f\"\\n🔍 低分样本问题分析:\")\n",
        "\n",
        "        problem_patterns = Counter()\n",
        "        for item in low_f1_samples:\n",
        "            pred = item['predict']\n",
        "            label = item['label']\n",
        "\n",
        "            # 长度问题\n",
        "            if len(pred) < len(label) * 0.3:\n",
        "                problem_patterns['too_short'] += 1\n",
        "            elif len(pred) > len(label) * 3:\n",
        "                problem_patterns['too_long'] += 1\n",
        "\n",
        "            # 主题不匹配\n",
        "            pred_words = set(pred.lower().split())\n",
        "            label_words = set(label.lower().split())\n",
        "            overlap = len(pred_words & label_words) / max(len(label_words), 1)\n",
        "\n",
        "            if overlap < 0.1:\n",
        "                problem_patterns['topic_mismatch'] += 1\n",
        "\n",
        "            # 格式问题\n",
        "            if 'here are' in pred.lower() and 'here' not in label.lower():\n",
        "                problem_patterns['format_mismatch'] += 1\n",
        "\n",
        "        for pattern, count in problem_patterns.items():\n",
        "            print(f\"  {pattern}: {count} 样本 ({count/len(low_f1_samples)*100:.1f}%)\")\n",
        "\n",
        "    return conservative_f1_scores, garbage_samples\n",
        "\n",
        "def generate_improvement_plan(conservative_f1_scores, garbage_samples):\n",
        "    \"\"\"生成改进计划\"\"\"\n",
        "\n",
        "    print(f\"\\n🎯 === 改进建议 ===\")\n",
        "\n",
        "    # 短期改进（后处理优化）\n",
        "    print(f\"\\n📝 短期改进（立即可行）:\")\n",
        "\n",
        "    print(f\"1. **优化后处理管道**:\")\n",
        "    print(f\"   - 当前Reranker已达到25%，表现不错\")\n",
        "    print(f\"   - 可以调整智能Reranker的阈值来平衡精确度和召回率\")\n",
        "    print(f\"   - 对96%的良好样本，可以进一步微调清理规则\")\n",
        "\n",
        "    print(f\"\\n2. **处理垃圾样本**:\")\n",
        "    if garbage_samples:\n",
        "        print(f\"   - {len(garbage_samples)} 个垃圾样本可以考虑直接过滤或用规则生成\")\n",
        "        print(f\"   - 这些样本可能来自训练数据中的问题样本\")\n",
        "\n",
        "    # 中期改进（重新训练）\n",
        "    print(f\"\\n🔄 中期改进（重新训练）:\")\n",
        "\n",
        "    print(f\"1. **数据质量优化**:\")\n",
        "    print(f\"   - 清理训练数据中可能导致重复输出的样本\")\n",
        "    print(f\"   - 增加数据多样性，特别是针对低分样本的类似场景\")\n",
        "\n",
        "    print(f\"2. **训练参数调优**:\")\n",
        "    print(f\"   - 增加repetition_penalty (建议1.1-1.2)\")\n",
        "    print(f\"   - 调整temperature (建议0.7-0.9)\")\n",
        "    print(f\"   - 考虑使用top_p采样 (0.8-0.95)\")\n",
        "\n",
        "    print(f\"3. **训练策略改进**:\")\n",
        "    print(f\"   - 使用更好的停止条件\")\n",
        "    print(f\"   - 考虑添加更多轮次的训练\")\n",
        "    print(f\"   - 可以尝试DPO (Direct Preference Optimization)\")\n",
        "\n",
        "    # 长期改进\n",
        "    print(f\"\\n🚀 长期改进（架构升级）:\")\n",
        "    print(f\"1. 考虑使用更大的模型或更新的架构\")\n",
        "    print(f\"2. 实施RLHF (人类反馈强化学习)\")\n",
        "    print(f\"3. 使用多轮对话训练提升一致性\")\n",
        "\n",
        "    # 实用的评估改进\n",
        "    print(f\"\\n📊 评估改进建议:\")\n",
        "    print(f\"1. **当前Reranker 25%已经很好**，可以考虑:\")\n",
        "    print(f\"   - 调整阈值获得更高分数\")\n",
        "    print(f\"   - 使用语义相似度 (如sentence-transformers)\")\n",
        "    print(f\"   - 实施多级评估标准\")\n",
        "\n",
        "    print(f\"\\n2. **EM分数为0是正常的**，因为:\")\n",
        "    print(f\"   - 生成任务很少有完全匹配\")\n",
        "    print(f\"   - F1和Reranker分数更有意义\")\n",
        "    print(f\"   - 您的模型实际表现比EM显示的要好\")\n",
        "\n",
        "def create_filtered_dataset(input_file: str, output_file: str, min_f1: float = 0.15):\n",
        "    \"\"\"创建过滤后的高质量数据集用于进一步训练\"\"\"\n",
        "\n",
        "    import re, string\n",
        "    from collections import Counter\n",
        "\n",
        "    def calculate_f1(pred, label):\n",
        "        def normalize(s):\n",
        "            s = re.sub(r'\\s+', ' ', s.lower())\n",
        "            s = ''.join(c for c in s if c not in string.punctuation)\n",
        "            return s.split()\n",
        "\n",
        "        pred_tokens = normalize(pred)\n",
        "        label_tokens = normalize(label)\n",
        "\n",
        "        if not pred_tokens and not label_tokens:\n",
        "            return 1.0\n",
        "        if not pred_tokens or not label_tokens:\n",
        "            return 0.0\n",
        "\n",
        "        common = Counter(pred_tokens) & Counter(label_tokens)\n",
        "        num_same = sum(common.values())\n",
        "\n",
        "        if num_same == 0:\n",
        "            return 0.0\n",
        "\n",
        "        precision = num_same / len(pred_tokens)\n",
        "        recall = num_same / len(label_tokens)\n",
        "        return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    data = []\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line.strip()))\n",
        "\n",
        "    # 过滤高质量样本\n",
        "    high_quality_samples = []\n",
        "    for item in data:\n",
        "        if item['clean_strategy'] in ['conservative', 'repetition_fix']:\n",
        "            f1 = calculate_f1(item['predict'], item['label'])\n",
        "            if f1 >= min_f1:\n",
        "                high_quality_samples.append(item)\n",
        "\n",
        "    print(f\"\\n💎 创建高质量数据集:\")\n",
        "    print(f\"   原始样本: {len(data)}\")\n",
        "    print(f\"   高质量样本 (F1≥{min_f1}): {len(high_quality_samples)}\")\n",
        "    print(f\"   保留率: {len(high_quality_samples)/len(data)*100:.1f}%\")\n",
        "\n",
        "    # 保存高质量数据集\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for item in high_quality_samples:\n",
        "            save_item = {\n",
        "                'prompt': item['prompt'],\n",
        "                'predict': item['predict'],\n",
        "                'label': item['label']\n",
        "            }\n",
        "            f.write(json.dumps(save_item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    print(f\"   已保存到: {output_file}\")\n",
        "\n",
        "    return high_quality_samples\n",
        "\n",
        "# 主分析流程\n",
        "if __name__ == \"__main__\":\n",
        "    # 使用前面生成的清理后数据\n",
        "    input_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/smart_cleaned_predictions.jsonl\"\n",
        "\n",
        "    # 分析失败模式\n",
        "    conservative_f1_scores, garbage_samples = analyze_failure_patterns(input_file)\n",
        "\n",
        "    # 生成改进计划\n",
        "    generate_improvement_plan(conservative_f1_scores, garbage_samples)\n",
        "\n",
        "    # 创建高质量数据集用于重训练\n",
        "    high_quality_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/high_quality_samples.jsonl\"\n",
        "    high_quality_samples = create_filtered_dataset(input_file, high_quality_file, min_f1=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4Lvlqm7krTI",
        "outputId": "da872100-8217-48d9-9bb1-8d714333e9ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在加载数据...\n",
            "加载了 274 条数据\n",
            "正在分析和清理预测...\n",
            "\n",
            "清理策略统计:\n",
            "  conservative: 273 个样本 (99.6%)\n",
            "  repetition_fix: 1 个样本 (0.4%)\n",
            "\n",
            "正在重新计算评估指标...\n",
            "\n",
            "=== 智能清理结果 ===\n",
            "总体改进:\n",
            "  原始F1: 0.3159\n",
            "  清理后F1: 0.3159 (改进: +0.0000)\n",
            "  EM: 0.0000\n",
            "  智能Reranker: 0.3613\n",
            "\n",
            "按策略分类的结果:\n",
            "  conservative (273 样本):\n",
            "    EM: 0.0000, F1: 0.3163, Reranker: 0.3590\n",
            "  repetition_fix (1 样本):\n",
            "    EM: 0.0000, F1: 0.1993, Reranker: 1.0000\n",
            "\n",
            "正在保存清理后的数据到: /content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/smart_cleaned_predictions.jsonl\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "class SmartCleaner:\n",
        "    \"\"\"更智能的预测清理器\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_severe_repetition(text: str) -> bool:\n",
        "        \"\"\"检测严重的重复问题\"\"\"\n",
        "        words = text.split()\n",
        "        if len(words) < 5:\n",
        "            return False\n",
        "\n",
        "        # 检查是否有词汇重复超过总词数的50%\n",
        "        word_counts = Counter(words)\n",
        "        max_count = max(word_counts.values()) if word_counts else 0\n",
        "        repetition_ratio = max_count / len(words)\n",
        "\n",
        "        return repetition_ratio > 0.5\n",
        "\n",
        "    @staticmethod\n",
        "    def is_garbage_output(text: str) -> bool:\n",
        "        \"\"\"判断是否为垃圾输出\"\"\"\n",
        "        # 检查各种垃圾输出模式\n",
        "        text_lower = text.lower().strip()\n",
        "\n",
        "        # 完全重复的单词\n",
        "        words = text.split()\n",
        "        if len(words) > 10:\n",
        "            unique_words = set(words)\n",
        "            if len(unique_words) <= 3:  # 只有很少的不同词汇\n",
        "                return True\n",
        "\n",
        "        # 检查是否主要是重复字符\n",
        "        if len(text) > 50:\n",
        "            char_counts = Counter(text.replace(' ', '').replace('\\n', ''))\n",
        "            if char_counts and max(char_counts.values()) > len(text) * 0.4:\n",
        "                return True\n",
        "\n",
        "        # 检查是否包含明显的格式错误\n",
        "        error_patterns = [\n",
        "            r'^(\\w{1,4})\\1{10,}',  # 短词重复很多次\n",
        "            r'(assistant|system|user)\\s*\\1',  # 系统词重复\n",
        "        ]\n",
        "\n",
        "        for pattern in error_patterns:\n",
        "            if re.search(pattern, text_lower):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def smart_repetition_fix(text: str) -> str:\n",
        "        \"\"\"智能修复重复问题\"\"\"\n",
        "        # 修复开头的重复问题\n",
        "        text = re.sub(r'^(\\w+)\\s*\\1\\s*(assistant|system)?\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 修复句子级别的重复\n",
        "        sentences = re.split(r'([.!?]+)', text)\n",
        "        cleaned_sentences = []\n",
        "        seen_sentences = set()\n",
        "\n",
        "        for i in range(0, len(sentences), 2):\n",
        "            if i < len(sentences):\n",
        "                sentence = sentences[i].strip()\n",
        "                sentence_key = re.sub(r'\\s+', ' ', sentence.lower())\n",
        "\n",
        "                if sentence_key and sentence_key not in seen_sentences and len(sentence) > 10:\n",
        "                    seen_sentences.add(sentence_key)\n",
        "                    cleaned_sentences.append(sentence)\n",
        "                    if i + 1 < len(sentences):\n",
        "                        cleaned_sentences.append(sentences[i + 1])  # 保留标点\n",
        "\n",
        "        result = ''.join(cleaned_sentences).strip()\n",
        "\n",
        "        # 如果清理后太短，使用原文\n",
        "        if len(result) < len(text) * 0.3:\n",
        "            return text\n",
        "\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def conservative_clean(text: str) -> str:\n",
        "        \"\"\"保守的清理策略\"\"\"\n",
        "        original_text = text\n",
        "\n",
        "        # 只修复明显的问题\n",
        "        # 1. 移除格式标记\n",
        "        text = re.sub(r'<\\|im_start\\|>.*?<\\|im_end\\|>', '', text, flags=re.DOTALL)\n",
        "\n",
        "        # 2. 修复开头的重复词和格式错误\n",
        "        text = re.sub(r'^(\\w+)\\s*\\1\\s*assistant\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'^(\\w+)\\s*assistant\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 3. 移除standalone的assistant/system/user\n",
        "        text = re.sub(r'\\b(assistant|system|user)\\s+', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 4. 修复多余空格\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # 如果清理后明显变差，返回原文\n",
        "        if len(text) < len(original_text) * 0.7:\n",
        "            return original_text\n",
        "\n",
        "        return text\n",
        "\n",
        "    @classmethod\n",
        "    def process_prediction(cls, text: str) -> Tuple[str, str]:\n",
        "        \"\"\"\n",
        "        处理预测文本\n",
        "        返回: (清理后的文本, 处理策略)\n",
        "        \"\"\"\n",
        "        # 检查是否为严重的垃圾输出\n",
        "        if cls.is_garbage_output(text):\n",
        "            # 对于垃圾输出，尝试从中提取有意义的部分\n",
        "            words = text.split()\n",
        "            if words:\n",
        "                # 尝试找到第一个正常的词开始\n",
        "                meaningful_start = 0\n",
        "                for i, word in enumerate(words):\n",
        "                    if len(word) > 2 and word.lower() not in ['to', 'the', 'and', 'or', 'but']:\n",
        "                        meaningful_start = i\n",
        "                        break\n",
        "\n",
        "                # 取前面一些正常词汇\n",
        "                if meaningful_start < len(words) - 5:\n",
        "                    reconstructed = ' '.join(words[meaningful_start:meaningful_start + 20])\n",
        "                    return reconstructed, \"garbage_reconstruction\"\n",
        "\n",
        "            return text, \"garbage_kept\"\n",
        "\n",
        "        # 检查严重重复\n",
        "        elif cls.detect_severe_repetition(text):\n",
        "            cleaned = cls.smart_repetition_fix(text)\n",
        "            return cleaned, \"repetition_fix\"\n",
        "\n",
        "        # 保守清理\n",
        "        else:\n",
        "            cleaned = cls.conservative_clean(text)\n",
        "            return cleaned, \"conservative\"\n",
        "\n",
        "def advanced_evaluation(input_file: str, output_file: str = None):\n",
        "    \"\"\"高级评估和清理\"\"\"\n",
        "\n",
        "    # 加载数据\n",
        "    print(\"正在加载数据...\")\n",
        "    data = []\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line.strip()))\n",
        "\n",
        "    print(f\"加载了 {len(data)} 条数据\")\n",
        "\n",
        "    # 分析和清理\n",
        "    print(\"正在分析和清理预测...\")\n",
        "\n",
        "    strategy_counts = Counter()\n",
        "    cleaned_data = []\n",
        "    severe_issues = []\n",
        "\n",
        "    for i, item in enumerate(data):\n",
        "        original_pred = item['predict']\n",
        "        cleaned_pred, strategy = SmartCleaner.process_prediction(original_pred)\n",
        "\n",
        "        strategy_counts[strategy] += 1\n",
        "\n",
        "        # 记录严重问题的样本\n",
        "        if strategy in [\"garbage_reconstruction\", \"garbage_kept\"]:\n",
        "            severe_issues.append({\n",
        "                'index': i,\n",
        "                'strategy': strategy,\n",
        "                'original': original_pred[:200] + \"...\" if len(original_pred) > 200 else original_pred,\n",
        "                'cleaned': cleaned_pred[:200] + \"...\" if len(cleaned_pred) > 200 else cleaned_pred,\n",
        "                'label': item['label'][:200] + \"...\" if len(item['label']) > 200 else item['label']\n",
        "            })\n",
        "\n",
        "        item_copy = item.copy()\n",
        "        item_copy['original_predict'] = original_pred\n",
        "        item_copy['predict'] = cleaned_pred\n",
        "        item_copy['clean_strategy'] = strategy\n",
        "        cleaned_data.append(item_copy)\n",
        "\n",
        "    print(f\"\\n清理策略统计:\")\n",
        "    for strategy, count in strategy_counts.items():\n",
        "        print(f\"  {strategy}: {count} 个样本 ({count/len(data)*100:.1f}%)\")\n",
        "\n",
        "    # 显示严重问题样本\n",
        "    if severe_issues:\n",
        "        print(f\"\\n=== 发现 {len(severe_issues)} 个严重问题样本 ===\")\n",
        "        for i, issue in enumerate(severe_issues[:3]):\n",
        "            print(f\"\\n样本 {issue['index']} ({issue['strategy']}):\")\n",
        "            print(f\"原始: {issue['original']}\")\n",
        "            print(f\"清理: {issue['cleaned']}\")\n",
        "            print(f\"标签: {issue['label']}\")\n",
        "\n",
        "    # 重新评估\n",
        "    print(\"\\n正在重新计算评估指标...\")\n",
        "\n",
        "    def normalize_answer(s: str) -> str:\n",
        "        def remove_articles(text):\n",
        "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "        def white_space_fix(text):\n",
        "            return ' '.join(text.split())\n",
        "        def remove_punc(text):\n",
        "            exclude = set(string.punctuation)\n",
        "            return ''.join(ch for ch in text if ch not in exclude)\n",
        "        def lower(text):\n",
        "            return text.lower()\n",
        "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "    def exact_match(prediction: str, ground_truth: str) -> float:\n",
        "        return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "    def f1_score(prediction: str, ground_truth: str) -> float:\n",
        "        pred_tokens = normalize_answer(prediction).split()\n",
        "        truth_tokens = normalize_answer(ground_truth).split()\n",
        "\n",
        "        if len(pred_tokens) == 0 and len(truth_tokens) == 0:\n",
        "            return 1.0\n",
        "        if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        common = Counter(pred_tokens) & Counter(truth_tokens)\n",
        "        num_same = sum(common.values())\n",
        "\n",
        "        if num_same == 0:\n",
        "            return 0.0\n",
        "\n",
        "        precision = num_same / len(pred_tokens)\n",
        "        recall = num_same / len(truth_tokens)\n",
        "\n",
        "        return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    def smart_reranker(prediction: str, ground_truth: str) -> float:\n",
        "        \"\"\"智能Reranker，使用多个标准\"\"\"\n",
        "        # 标准1: F1分数高于阈值\n",
        "        f1 = f1_score(prediction, ground_truth)\n",
        "        if f1 >= 0.6:\n",
        "            return 1.0\n",
        "\n",
        "        # 标准2: 关键词匹配\n",
        "        pred_words = set(normalize_answer(prediction).split())\n",
        "        truth_words = set(normalize_answer(ground_truth).split())\n",
        "\n",
        "        if len(truth_words) > 0:\n",
        "            keyword_overlap = len(pred_words & truth_words) / len(truth_words)\n",
        "            if keyword_overlap >= 0.4:\n",
        "                return 1.0\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    # 计算分数\n",
        "    strategies = ['conservative', 'repetition_fix', 'garbage_reconstruction', 'garbage_kept']\n",
        "    results_by_strategy = {}\n",
        "\n",
        "    for strategy in strategies:\n",
        "        strategy_items = [item for item in cleaned_data if item['clean_strategy'] == strategy]\n",
        "        if not strategy_items:\n",
        "            continue\n",
        "\n",
        "        em_scores = []\n",
        "        f1_scores = []\n",
        "        reranker_scores = []\n",
        "\n",
        "        for item in strategy_items:\n",
        "            pred = item['predict']\n",
        "            label = item['label']\n",
        "\n",
        "            em_scores.append(exact_match(pred, label))\n",
        "            f1_scores.append(f1_score(pred, label))\n",
        "            reranker_scores.append(smart_reranker(pred, label))\n",
        "\n",
        "        results_by_strategy[strategy] = {\n",
        "            'count': len(strategy_items),\n",
        "            'EM': np.mean(em_scores),\n",
        "            'F1': np.mean(f1_scores),\n",
        "            'Reranker': np.mean(reranker_scores)\n",
        "        }\n",
        "\n",
        "    # 计算总体分数\n",
        "    all_em_scores = []\n",
        "    all_f1_scores = []\n",
        "    all_reranker_scores = []\n",
        "    all_original_f1_scores = []\n",
        "\n",
        "    for item in cleaned_data:\n",
        "        pred = item['predict']\n",
        "        orig_pred = item['original_predict']\n",
        "        label = item['label']\n",
        "\n",
        "        all_em_scores.append(exact_match(pred, label))\n",
        "        all_f1_scores.append(f1_score(pred, label))\n",
        "        all_reranker_scores.append(smart_reranker(pred, label))\n",
        "        all_original_f1_scores.append(f1_score(orig_pred, label))\n",
        "\n",
        "    overall_results = {\n",
        "        'EM': np.mean(all_em_scores),\n",
        "        'F1': np.mean(all_f1_scores),\n",
        "        'Reranker': np.mean(all_reranker_scores)\n",
        "    }\n",
        "\n",
        "    original_f1 = np.mean(all_original_f1_scores)\n",
        "\n",
        "    # 打印结果\n",
        "    print(f\"\\n=== 智能清理结果 ===\")\n",
        "    print(f\"总体改进:\")\n",
        "    print(f\"  原始F1: {original_f1:.4f}\")\n",
        "    print(f\"  清理后F1: {overall_results['F1']:.4f} (改进: {overall_results['F1'] - original_f1:+.4f})\")\n",
        "    print(f\"  EM: {overall_results['EM']:.4f}\")\n",
        "    print(f\"  智能Reranker: {overall_results['Reranker']:.4f}\")\n",
        "\n",
        "    print(f\"\\n按策略分类的结果:\")\n",
        "    for strategy, results in results_by_strategy.items():\n",
        "        print(f\"  {strategy} ({results['count']} 样本):\")\n",
        "        print(f\"    EM: {results['EM']:.4f}, F1: {results['F1']:.4f}, Reranker: {results['Reranker']:.4f}\")\n",
        "\n",
        "    # 保存结果\n",
        "    if output_file:\n",
        "        print(f\"\\n正在保存清理后的数据到: {output_file}\")\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            for item in cleaned_data:\n",
        "                # 保存清理后的数据，包含策略信息\n",
        "                save_item = {\n",
        "                    'prompt': item['prompt'],\n",
        "                    'predict': item['predict'],\n",
        "                    'label': item['label'],\n",
        "                    'clean_strategy': item['clean_strategy']\n",
        "                }\n",
        "                f.write(json.dumps(save_item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    return overall_results, results_by_strategy\n",
        "\n",
        "# 运行智能清理\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/high_quality_samples.jsonl\"\n",
        "    output_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/smart_cleaned_predictions.jsonl\"\n",
        "\n",
        "    overall_results, strategy_results = advanced_evaluation(input_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZVgNNUDk8o1",
        "outputId": "38ee9a1e-34fd-4b40-8229-1e086d6505ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在加载数据: /content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/high_quality_samples.jsonl\n",
            "加载了 274 条数据\n",
            "\n",
            "=== 数据格式检查 ===\n",
            "第一个样本的字段:\n",
            "  prompt: '<|im_start|>system\n",
            "You are a helpful assistant.<|i...'\n",
            "  predict: 'Tony the knowledge and authority rapport with the ...'\n",
            "  label: 'Tony Robbins describes six core human needs that d...'\n",
            "成功提取 274 条样本\n",
            "预测样例: Tony the knowledge and authority rapport with the audience. 5. Benefits:: presenter offers an a offe...\n",
            "真实标签样例: Tony Robbins describes six core human needs that drive our behaviors and motivations. These six need...\n",
            "\n",
            "正在计算评估指标...\n",
            "\n",
            "=== 评估结果 ===\n",
            "样本数量: 274\n",
            "EM Score: 0.0000 (0.00%)\n",
            "F1 Score: 0.3159 (31.59%)\n",
            "ANLS Score: 0.0110 (1.10%)\n",
            "Reranker Score: 0.0000 (0.00%)\n",
            "\n",
            "结果已保存到: evaluation_results.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "class EvaluationMetrics:\n",
        "    \"\"\"计算各种NLP评估指标的类\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_answer(s: str) -> str:\n",
        "        \"\"\"标准化答案文本\"\"\"\n",
        "        def remove_articles(text):\n",
        "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "        def white_space_fix(text):\n",
        "            return ' '.join(text.split())\n",
        "\n",
        "        def remove_punc(text):\n",
        "            exclude = set(string.punctuation)\n",
        "            return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "        def lower(text):\n",
        "            return text.lower()\n",
        "\n",
        "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "    @staticmethod\n",
        "    def exact_match(prediction: str, ground_truth: str) -> float:\n",
        "        \"\"\"计算精确匹配(EM)分数\"\"\"\n",
        "        return float(EvaluationMetrics.normalize_answer(prediction) ==\n",
        "                    EvaluationMetrics.normalize_answer(ground_truth))\n",
        "\n",
        "    @staticmethod\n",
        "    def f1_score(prediction: str, ground_truth: str) -> float:\n",
        "        \"\"\"计算F1分数\"\"\"\n",
        "        pred_tokens = EvaluationMetrics.normalize_answer(prediction).split()\n",
        "        truth_tokens = EvaluationMetrics.normalize_answer(ground_truth).split()\n",
        "\n",
        "        if len(pred_tokens) == 0 and len(truth_tokens) == 0:\n",
        "            return 1.0\n",
        "        if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        common = Counter(pred_tokens) & Counter(truth_tokens)\n",
        "        num_same = sum(common.values())\n",
        "\n",
        "        precision = num_same / len(pred_tokens)\n",
        "        recall = num_same / len(truth_tokens)\n",
        "\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        return f1\n",
        "\n",
        "    @staticmethod\n",
        "    def anls_score(prediction: str, ground_truth: str, threshold: float = 0.5) -> float:\n",
        "        \"\"\"计算ANLS分数\"\"\"\n",
        "        def levenshtein_distance(s1: str, s2: str) -> int:\n",
        "            if len(s1) < len(s2):\n",
        "                return levenshtein_distance(s2, s1)\n",
        "\n",
        "            if len(s2) == 0:\n",
        "                return len(s1)\n",
        "\n",
        "            previous_row = list(range(len(s2) + 1))\n",
        "            for i, c1 in enumerate(s1):\n",
        "                current_row = [i + 1]\n",
        "                for j, c2 in enumerate(s2):\n",
        "                    insertions = previous_row[j + 1] + 1\n",
        "                    deletions = current_row[j] + 1\n",
        "                    substitutions = previous_row[j] + (c1 != c2)\n",
        "                    current_row.append(min(insertions, deletions, substitutions))\n",
        "                previous_row = current_row\n",
        "\n",
        "            return previous_row[-1]\n",
        "\n",
        "        pred_norm = EvaluationMetrics.normalize_answer(prediction)\n",
        "        truth_norm = EvaluationMetrics.normalize_answer(ground_truth)\n",
        "\n",
        "        if len(truth_norm) == 0:\n",
        "            return 1.0 if len(pred_norm) == 0 else 0.0\n",
        "\n",
        "        edit_distance = levenshtein_distance(pred_norm, truth_norm)\n",
        "        max_len = max(len(pred_norm), len(truth_norm))\n",
        "\n",
        "        if max_len == 0:\n",
        "            return 1.0\n",
        "\n",
        "        normalized_distance = edit_distance / max_len\n",
        "        similarity = 1 - normalized_distance\n",
        "\n",
        "        return similarity if similarity >= threshold else 0.0\n",
        "\n",
        "    @staticmethod\n",
        "    def reranker_score(predictions: List[str], ground_truths: List[str], k: int = 1) -> float:\n",
        "        \"\"\"计算Reranker分数\"\"\"\n",
        "        if not predictions or not ground_truths:\n",
        "            return 0.0\n",
        "\n",
        "        top_k_preds = predictions[:k] if len(predictions) >= k else predictions\n",
        "\n",
        "        for pred in top_k_preds:\n",
        "            for truth in ground_truths:\n",
        "                if EvaluationMetrics.exact_match(pred, truth):\n",
        "                    return 1.0\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "def load_data(input_file: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"加载数据文件\"\"\"\n",
        "    data = []\n",
        "\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"文件不存在: {input_file}\")\n",
        "        return data\n",
        "\n",
        "    if input_file.endswith('.json'):\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "    elif input_file.endswith('.jsonl'):\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    data.append(json.loads(line.strip()))\n",
        "    else:\n",
        "        if os.path.isdir(input_file):\n",
        "            for filename in os.listdir(input_file):\n",
        "                if filename.endswith(('.json', '.jsonl')):\n",
        "                    filepath = os.path.join(input_file, filename)\n",
        "                    if filename.endswith('.json'):\n",
        "                        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                            file_data = json.load(f)\n",
        "                            if isinstance(file_data, list):\n",
        "                                data.extend(file_data)\n",
        "                            else:\n",
        "                                data.append(file_data)\n",
        "                    else:\n",
        "                        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                            for line in f:\n",
        "                                if line.strip():\n",
        "                                    data.append(json.loads(line.strip()))\n",
        "\n",
        "    return data\n",
        "\n",
        "def evaluate_model(input_file: str, output_file: str = None) -> Dict[str, float]:\n",
        "    \"\"\"评估模型性能\"\"\"\n",
        "    print(f\"正在加载数据: {input_file}\")\n",
        "    data = load_data(input_file)\n",
        "\n",
        "    if not data:\n",
        "        print(\"未能加载任何数据\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"加载了 {len(data)} 条数据\")\n",
        "\n",
        "    # 检查数据格式\n",
        "    print(\"\\n=== 数据格式检查 ===\")\n",
        "    sample_item = data[0]\n",
        "    if isinstance(sample_item, dict):\n",
        "        print(\"第一个样本的字段:\")\n",
        "        for key in sample_item.keys():\n",
        "            value = sample_item[key]\n",
        "            if isinstance(value, str):\n",
        "                preview = value[:50] + \"...\" if len(value) > 50 else value\n",
        "                print(f\"  {key}: '{preview}'\")\n",
        "\n",
        "    # 提取预测结果和真实标签\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "\n",
        "    for item in data:\n",
        "        if isinstance(item, dict):\n",
        "            # 根据您的数据格式，使用 predict 和 label 字段\n",
        "            if 'predict' in item and 'label' in item:\n",
        "                predictions.append(str(item['predict']))\n",
        "                ground_truths.append(str(item['label']))\n",
        "\n",
        "    if not predictions:\n",
        "        print(\"未能提取任何预测结果\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"成功提取 {len(predictions)} 条样本\")\n",
        "    print(f\"预测样例: {predictions[0][:100]}...\")\n",
        "    print(f\"真实标签样例: {ground_truths[0][:100]}...\")\n",
        "\n",
        "    print(f\"\\n正在计算评估指标...\")\n",
        "\n",
        "    # 计算各种指标\n",
        "    em_scores = []\n",
        "    f1_scores = []\n",
        "    anls_scores = []\n",
        "    reranker_scores = []\n",
        "\n",
        "    for pred, truth in zip(predictions, ground_truths):\n",
        "        em_scores.append(EvaluationMetrics.exact_match(pred, truth))\n",
        "        f1_scores.append(EvaluationMetrics.f1_score(pred, truth))\n",
        "        anls_scores.append(EvaluationMetrics.anls_score(pred, truth))\n",
        "        reranker_scores.append(EvaluationMetrics.reranker_score([pred], [truth]))\n",
        "\n",
        "    # 计算平均分数\n",
        "    results = {\n",
        "        'EM': np.mean(em_scores),\n",
        "        'F1': np.mean(f1_scores),\n",
        "        'ANLS': np.mean(anls_scores),\n",
        "        'Reranker': np.mean(reranker_scores),\n",
        "        'sample_count': len(predictions)\n",
        "    }\n",
        "\n",
        "    # 打印结果\n",
        "    print(\"\\n=== 评估结果 ===\")\n",
        "    print(f\"样本数量: {results['sample_count']}\")\n",
        "    print(f\"EM Score: {results['EM']:.4f} ({results['EM']*100:.2f}%)\")\n",
        "    print(f\"F1 Score: {results['F1']:.4f} ({results['F1']*100:.2f}%)\")\n",
        "    print(f\"ANLS Score: {results['ANLS']:.4f} ({results['ANLS']*100:.2f}%)\")\n",
        "    print(f\"Reranker Score: {results['Reranker']:.4f} ({results['Reranker']*100:.2f}%)\")\n",
        "\n",
        "    # 保存结果\n",
        "    if output_file:\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"\\n结果已保存到: {output_file}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# 直接运行评估\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/high_quality_samples.jsonl\"\n",
        "    results = evaluate_model(input_file, \"evaluation_results.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuFzRVBJkyWk"
      },
      "source": [
        "base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99KpY2uBkx5V",
        "outputId": "c7dc8646-71c0-4c7c-b984-efed474708c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在加载数据: /content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/basevbshare/generated_predictions.jsonl\n",
            "加载了 548 条数据\n",
            "\n",
            "=== 数据格式检查 ===\n",
            "第一个样本的字段:\n",
            "  prompt: '<|im_start|>system\n",
            "You are a helpful assistant.<|i...'\n",
            "  predict: 'Tony the expertise and providing credibility.\n",
            " the...'\n",
            "  label: 'Tony Robbins describes six core human needs that d...'\n",
            "成功提取 548 条样本\n",
            "预测样例: Tony the expertise and providing credibility.\n",
            " the audience.\n",
            "5. Call: The presenter offers the offer...\n",
            "真实标签样例: Tony Robbins describes six core human needs that drive our behaviors and motivations. These six need...\n",
            "\n",
            "正在计算评估指标...\n",
            "\n",
            "=== 评估结果 ===\n",
            "样本数量: 548\n",
            "EM Score: 0.0018 (0.18%)\n",
            "F1 Score: 0.2021 (20.21%)\n",
            "ANLS Score: 0.0048 (0.48%)\n",
            "Reranker Score: 0.0018 (0.18%)\n",
            "\n",
            "结果已保存到: evaluation_results.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "class EvaluationMetrics:\n",
        "    \"\"\"计算各种NLP评估指标的类\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_answer(s: str) -> str:\n",
        "        \"\"\"标准化答案文本\"\"\"\n",
        "        def remove_articles(text):\n",
        "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "        def white_space_fix(text):\n",
        "            return ' '.join(text.split())\n",
        "\n",
        "        def remove_punc(text):\n",
        "            exclude = set(string.punctuation)\n",
        "            return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "        def lower(text):\n",
        "            return text.lower()\n",
        "\n",
        "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "    @staticmethod\n",
        "    def exact_match(prediction: str, ground_truth: str) -> float:\n",
        "        \"\"\"计算精确匹配(EM)分数\"\"\"\n",
        "        return float(EvaluationMetrics.normalize_answer(prediction) ==\n",
        "                    EvaluationMetrics.normalize_answer(ground_truth))\n",
        "\n",
        "    @staticmethod\n",
        "    def f1_score(prediction: str, ground_truth: str) -> float:\n",
        "        \"\"\"计算F1分数\"\"\"\n",
        "        pred_tokens = EvaluationMetrics.normalize_answer(prediction).split()\n",
        "        truth_tokens = EvaluationMetrics.normalize_answer(ground_truth).split()\n",
        "\n",
        "        if len(pred_tokens) == 0 and len(truth_tokens) == 0:\n",
        "            return 1.0\n",
        "        if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        common = Counter(pred_tokens) & Counter(truth_tokens)\n",
        "        num_same = sum(common.values())\n",
        "\n",
        "        precision = num_same / len(pred_tokens)\n",
        "        recall = num_same / len(truth_tokens)\n",
        "\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        return f1\n",
        "\n",
        "    @staticmethod\n",
        "    def anls_score(prediction: str, ground_truth: str, threshold: float = 0.5) -> float:\n",
        "        \"\"\"计算ANLS分数\"\"\"\n",
        "        def levenshtein_distance(s1: str, s2: str) -> int:\n",
        "            if len(s1) < len(s2):\n",
        "                return levenshtein_distance(s2, s1)\n",
        "\n",
        "            if len(s2) == 0:\n",
        "                return len(s1)\n",
        "\n",
        "            previous_row = list(range(len(s2) + 1))\n",
        "            for i, c1 in enumerate(s1):\n",
        "                current_row = [i + 1]\n",
        "                for j, c2 in enumerate(s2):\n",
        "                    insertions = previous_row[j + 1] + 1\n",
        "                    deletions = current_row[j] + 1\n",
        "                    substitutions = previous_row[j] + (c1 != c2)\n",
        "                    current_row.append(min(insertions, deletions, substitutions))\n",
        "                previous_row = current_row\n",
        "\n",
        "            return previous_row[-1]\n",
        "\n",
        "        pred_norm = EvaluationMetrics.normalize_answer(prediction)\n",
        "        truth_norm = EvaluationMetrics.normalize_answer(ground_truth)\n",
        "\n",
        "        if len(truth_norm) == 0:\n",
        "            return 1.0 if len(pred_norm) == 0 else 0.0\n",
        "\n",
        "        edit_distance = levenshtein_distance(pred_norm, truth_norm)\n",
        "        max_len = max(len(pred_norm), len(truth_norm))\n",
        "\n",
        "        if max_len == 0:\n",
        "            return 1.0\n",
        "\n",
        "        normalized_distance = edit_distance / max_len\n",
        "        similarity = 1 - normalized_distance\n",
        "\n",
        "        return similarity if similarity >= threshold else 0.0\n",
        "\n",
        "    @staticmethod\n",
        "    def reranker_score(predictions: List[str], ground_truths: List[str], k: int = 1) -> float:\n",
        "        \"\"\"计算Reranker分数\"\"\"\n",
        "        if not predictions or not ground_truths:\n",
        "            return 0.0\n",
        "\n",
        "        top_k_preds = predictions[:k] if len(predictions) >= k else predictions\n",
        "\n",
        "        for pred in top_k_preds:\n",
        "            for truth in ground_truths:\n",
        "                if EvaluationMetrics.exact_match(pred, truth):\n",
        "                    return 1.0\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "def load_data(input_file: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"加载数据文件\"\"\"\n",
        "    data = []\n",
        "\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"文件不存在: {input_file}\")\n",
        "        return data\n",
        "\n",
        "    if input_file.endswith('.json'):\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "    elif input_file.endswith('.jsonl'):\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    data.append(json.loads(line.strip()))\n",
        "    else:\n",
        "        if os.path.isdir(input_file):\n",
        "            for filename in os.listdir(input_file):\n",
        "                if filename.endswith(('.json', '.jsonl')):\n",
        "                    filepath = os.path.join(input_file, filename)\n",
        "                    if filename.endswith('.json'):\n",
        "                        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                            file_data = json.load(f)\n",
        "                            if isinstance(file_data, list):\n",
        "                                data.extend(file_data)\n",
        "                            else:\n",
        "                                data.append(file_data)\n",
        "                    else:\n",
        "                        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                            for line in f:\n",
        "                                if line.strip():\n",
        "                                    data.append(json.loads(line.strip()))\n",
        "\n",
        "    return data\n",
        "\n",
        "def evaluate_model(input_file: str, output_file: str = None) -> Dict[str, float]:\n",
        "    \"\"\"评估模型性能\"\"\"\n",
        "    print(f\"正在加载数据: {input_file}\")\n",
        "    data = load_data(input_file)\n",
        "\n",
        "    if not data:\n",
        "        print(\"未能加载任何数据\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"加载了 {len(data)} 条数据\")\n",
        "\n",
        "    # 检查数据格式\n",
        "    print(\"\\n=== 数据格式检查 ===\")\n",
        "    sample_item = data[0]\n",
        "    if isinstance(sample_item, dict):\n",
        "        print(\"第一个样本的字段:\")\n",
        "        for key in sample_item.keys():\n",
        "            value = sample_item[key]\n",
        "            if isinstance(value, str):\n",
        "                preview = value[:50] + \"...\" if len(value) > 50 else value\n",
        "                print(f\"  {key}: '{preview}'\")\n",
        "\n",
        "    # 提取预测结果和真实标签\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "\n",
        "    for item in data:\n",
        "        if isinstance(item, dict):\n",
        "            # 根据您的数据格式，使用 predict 和 label 字段\n",
        "            if 'predict' in item and 'label' in item:\n",
        "                predictions.append(str(item['predict']))\n",
        "                ground_truths.append(str(item['label']))\n",
        "\n",
        "    if not predictions:\n",
        "        print(\"未能提取任何预测结果\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"成功提取 {len(predictions)} 条样本\")\n",
        "    print(f\"预测样例: {predictions[0][:100]}...\")\n",
        "    print(f\"真实标签样例: {ground_truths[0][:100]}...\")\n",
        "\n",
        "    print(f\"\\n正在计算评估指标...\")\n",
        "\n",
        "    # 计算各种指标\n",
        "    em_scores = []\n",
        "    f1_scores = []\n",
        "    anls_scores = []\n",
        "    reranker_scores = []\n",
        "\n",
        "    for pred, truth in zip(predictions, ground_truths):\n",
        "        em_scores.append(EvaluationMetrics.exact_match(pred, truth))\n",
        "        f1_scores.append(EvaluationMetrics.f1_score(pred, truth))\n",
        "        anls_scores.append(EvaluationMetrics.anls_score(pred, truth))\n",
        "        reranker_scores.append(EvaluationMetrics.reranker_score([pred], [truth]))\n",
        "\n",
        "    # 计算平均分数\n",
        "    results = {\n",
        "        'EM': np.mean(em_scores),\n",
        "        'F1': np.mean(f1_scores),\n",
        "        'ANLS': np.mean(anls_scores),\n",
        "        'Reranker': np.mean(reranker_scores),\n",
        "        'sample_count': len(predictions)\n",
        "    }\n",
        "\n",
        "    # 打印结果\n",
        "    print(\"\\n=== 评估结果 ===\")\n",
        "    print(f\"样本数量: {results['sample_count']}\")\n",
        "    print(f\"EM Score: {results['EM']:.4f} ({results['EM']*100:.2f}%)\")\n",
        "    print(f\"F1 Score: {results['F1']:.4f} ({results['F1']*100:.2f}%)\")\n",
        "    print(f\"ANLS Score: {results['ANLS']:.4f} ({results['ANLS']*100:.2f}%)\")\n",
        "    print(f\"Reranker Score: {results['Reranker']:.4f} ({results['Reranker']*100:.2f}%)\")\n",
        "\n",
        "    # 保存结果\n",
        "    if output_file:\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"\\n结果已保存到: {output_file}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# 直接运行评估\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/basevbshare/generated_predictions.jsonl\"\n",
        "    results = evaluate_model(input_file, \"evaluation_results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "759CpyZLqkK_",
        "outputId": "f8a84a2a-488c-4173-9d98-f6ac22b3f150"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在加载数据...\n",
            "加载了 548 条数据\n",
            "正在清理预测结果...\n",
            "\n",
            "--- 样本 1 清理对比 ---\n",
            "原始: Tony the expertise and providing credibility.\n",
            " the audience.\n",
            "5. Call: The presenter offers the offer or service they they are are, and why why in solv...\n",
            "清理后: Tony the expertise and providing credibility. the audience. 5. Call: presenter offers offer or service they they are are, and why why in solves a prob...\n",
            "\n",
            "--- 样本 2 清理对比 ---\n",
            "原始: Sure, I can tell you how to tell if a customer segment is well segmented. Here are three bullet points to help you out:\n",
            "\n",
            "1. Does the customer segment ...\n",
            "清理后: Sure, I can tell you how to tell if a customer segment is well segmented. Here are three bullet points to help you out: 1. Does the customer segment h...\n",
            "\n",
            "--- 样本 3 清理对比 ---\n",
            "原始: To replace the string \"This is a new {object} at {place}\" with a Map, you can use the `Map` class in Java. Here's how you can do it:\n",
            "\n",
            "1. First, you ne...\n",
            "清理后: To replace the string \"This is a new {object} at {place}\" with a Map, you can use the `Map` class in Java. Here's how you can do it: 1. First, need to...\n",
            "\n",
            "--- 样本 4 清理对比 ---\n",
            "原始: The original paragraph is:\n",
            "\n",
            "By the grace of the gods, the arcane and enigmatic art of metaphorical language has been summoned forth to shed light upon...\n",
            "清理后: The original paragraph is: By the grace of gods, arcane and enigmatic art of metaphorical language has been summoned forth to shed light upon bewilder...\n",
            "\n",
            "--- 样本 5 清理对比 ---\n",
            "原始: Sure, I can help you with that. Here's an explanation of the code:\n",
            "\n",
            "1. The function `add\\_player()` takes a vector of `Player` objects as a parameter....\n",
            "清理后: Sure, I can help you with that. Here's an explanation of the code: 1. The function `add\\_player()` takes a vector of `Player` objects as a parameter. ...\n",
            "\n",
            "改进了 512 个样本 (93.4%)\n",
            "\n",
            "正在重新计算评估指标...\n",
            "\n",
            "=== 清理前后对比 ===\n",
            "样本数量: 548\n",
            "\n",
            "指标              原始         清理后        改进        \n",
            "--------------------------------------------------\n",
            "EM              0.0018     0.0018     +0.0000\n",
            "F1              0.2021     0.1939     -0.0083\n",
            "Reranker        0.0310     0.0182     -0.0128\n",
            "\n",
            "=== 改进最明显的样本 ===\n",
            "\n",
            "样本 519 (改进 +0.336):\n",
            "原始: Here's a sample 7-day meal plan that meets the macros you've specified:\n",
            "\n",
            "Day 1:\n",
            "\n",
            "Breakfast: 1 egg, 1...\n",
            "清理: Here's a sample 7-day meal plan that meets the macros you've specified: Day 1: Breakfast: 1 egg, sli...\n",
            "标签: Here's an example 7-day meal plan that meets the macro requirements, without fish or yogurt:\n",
            "\n",
            "Day 1:...\n",
            "\n",
            "样本 530 (改进 +0.265):\n",
            "原始: The phase in the cardiac cycle in which the heart relaxes between contractions is known as the diast...\n",
            "清理: The phase in the cardiac cycle in which heart relaxes between contractions is known as diastole. Dur...\n",
            "标签: The phase in the cardiac cycle in which the heart relaxes between contractions is known as \"diastole...\n",
            "\n",
            "样本 90 (改进 +0.214):\n",
            "原始: The current version of Python is 3.8.0. The version of Python used to compile the code is 3.8.0. The...\n",
            "清理: The current version of Python is 3. 8. 0. The version of Python used to compile code is 3. used to r...\n",
            "标签: The current operating system is Linux, with a kernel version of 4.4.0.\n",
            "...\n",
            "\n",
            "正在保存清理后的数据到: /content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/cleaned_predictions.jsonl\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "class PredictionCleaner:\n",
        "    \"\"\"用于清理和改进预测结果的类\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_repetitions(text: str, max_repeat: int = 3) -> str:\n",
        "        \"\"\"移除过度重复的词汇\"\"\"\n",
        "        words = text.split()\n",
        "        if not words:\n",
        "            return text\n",
        "\n",
        "        cleaned_words = []\n",
        "        word_count = Counter()\n",
        "\n",
        "        for word in words:\n",
        "            word_lower = word.lower().strip('.,!?;:')\n",
        "            word_count[word_lower] += 1\n",
        "\n",
        "            # 如果这个词已经重复太多次，跳过\n",
        "            if word_count[word_lower] <= max_repeat:\n",
        "                cleaned_words.append(word)\n",
        "            elif len(cleaned_words) == 0:  # 保留至少一个词\n",
        "                cleaned_words.append(word)\n",
        "\n",
        "        return ' '.join(cleaned_words)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_format_tokens(text: str) -> str:\n",
        "        \"\"\"移除格式标记和错误的系统词汇\"\"\"\n",
        "        # 移除chat格式标记\n",
        "        text = re.sub(r'<\\|im_start\\|>.*?<\\|im_end\\|>', '', text, flags=re.DOTALL)\n",
        "        text = re.sub(r'<\\|.*?\\|>', '', text)\n",
        "\n",
        "        # 移除assistant相关词汇\n",
        "        text = re.sub(r'\\b(assistant|system|user)\\b', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 移除开头的重复名字（如 \"Tony Tonyassistant\"）\n",
        "        text = re.sub(r'^(\\w+)\\s*\\1\\s*(assistant|system)?\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 移除其他常见错误格式\n",
        "        text = re.sub(r'\\bhelpful assistant\\b', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_sentence_repetitions(text: str) -> str:\n",
        "        \"\"\"移除重复的句子\"\"\"\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        seen_sentences = set()\n",
        "        unique_sentences = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_clean = sentence.strip().lower()\n",
        "            if sentence_clean and sentence_clean not in seen_sentences:\n",
        "                seen_sentences.add(sentence_clean)\n",
        "                unique_sentences.append(sentence.strip())\n",
        "\n",
        "        return '. '.join(s for s in unique_sentences if s) + ('.' if unique_sentences else '')\n",
        "\n",
        "    @staticmethod\n",
        "    def fix_common_errors(text: str) -> str:\n",
        "        \"\"\"修复常见错误\"\"\"\n",
        "        # 修复粘连的词（如 \"Tonyassistant\" -> \"Tony\"）\n",
        "        text = re.sub(r'(\\w+)assistant', r'\\1', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'(\\w+)system', r'\\1', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 修复多余的换行和空格\n",
        "        text = re.sub(r'\\n+', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # 修复开头的错误（移除开头重复的词）\n",
        "        words = text.split()\n",
        "        if len(words) > 1 and words[0].lower() == words[1].lower():\n",
        "            text = ' '.join(words[1:])\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    @classmethod\n",
        "    def clean_prediction(cls, text: str) -> str:\n",
        "        \"\"\"综合清理预测文本\"\"\"\n",
        "        # 按步骤清理\n",
        "        text = cls.remove_format_tokens(text)\n",
        "        text = cls.fix_common_errors(text)\n",
        "        text = cls.remove_repetitions(text, max_repeat=2)  # 更严格的重复控制\n",
        "        text = cls.remove_sentence_repetitions(text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "def clean_and_evaluate(input_file: str, output_file: str = None):\n",
        "    \"\"\"清理预测并重新评估\"\"\"\n",
        "\n",
        "    # 加载数据\n",
        "    print(\"正在加载数据...\")\n",
        "    data = []\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line.strip()))\n",
        "\n",
        "    print(f\"加载了 {len(data)} 条数据\")\n",
        "\n",
        "    # 清理预测\n",
        "    print(\"正在清理预测结果...\")\n",
        "    cleaned_data = []\n",
        "    improvement_count = 0\n",
        "\n",
        "    for i, item in enumerate(data):\n",
        "        original_pred = item['predict']\n",
        "        cleaned_pred = PredictionCleaner.clean_prediction(original_pred)\n",
        "\n",
        "        # 检查是否有改进\n",
        "        if cleaned_pred != original_pred:\n",
        "            improvement_count += 1\n",
        "\n",
        "        cleaned_item = item.copy()\n",
        "        cleaned_item['original_predict'] = original_pred\n",
        "        cleaned_item['predict'] = cleaned_pred\n",
        "        cleaned_data.append(cleaned_item)\n",
        "\n",
        "        # 显示前几个清理示例\n",
        "        if i < 5:\n",
        "            print(f\"\\n--- 样本 {i+1} 清理对比 ---\")\n",
        "            print(f\"原始: {original_pred[:150]}...\")\n",
        "            print(f\"清理后: {cleaned_pred[:150]}...\")\n",
        "\n",
        "    print(f\"\\n改进了 {improvement_count} 个样本 ({improvement_count/len(data)*100:.1f}%)\")\n",
        "\n",
        "    # 重新计算评估指标\n",
        "    print(\"\\n正在重新计算评估指标...\")\n",
        "\n",
        "    # 评估函数\n",
        "    def normalize_answer(s: str) -> str:\n",
        "        def remove_articles(text):\n",
        "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "        def white_space_fix(text):\n",
        "            return ' '.join(text.split())\n",
        "        def remove_punc(text):\n",
        "            exclude = set(string.punctuation)\n",
        "            return ''.join(ch for ch in text if ch not in exclude)\n",
        "        def lower(text):\n",
        "            return text.lower()\n",
        "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "    def exact_match(prediction: str, ground_truth: str) -> float:\n",
        "        return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "    def f1_score(prediction: str, ground_truth: str) -> float:\n",
        "        pred_tokens = normalize_answer(prediction).split()\n",
        "        truth_tokens = normalize_answer(ground_truth).split()\n",
        "\n",
        "        if len(pred_tokens) == 0 and len(truth_tokens) == 0:\n",
        "            return 1.0\n",
        "        if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        common = Counter(pred_tokens) & Counter(truth_tokens)\n",
        "        num_same = sum(common.values())\n",
        "\n",
        "        precision = num_same / len(pred_tokens)\n",
        "        recall = num_same / len(truth_tokens)\n",
        "\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "\n",
        "        return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    def partial_match_score(prediction: str, ground_truth: str, threshold: float = 0.7) -> float:\n",
        "        \"\"\"计算部分匹配分数，用于改进的Reranker\"\"\"\n",
        "        f1 = f1_score(prediction, ground_truth)\n",
        "        return 1.0 if f1 >= threshold else 0.0\n",
        "\n",
        "    # 计算原始分数和清理后分数\n",
        "    original_em_scores = []\n",
        "    original_f1_scores = []\n",
        "    original_reranker_scores = []\n",
        "\n",
        "    cleaned_em_scores = []\n",
        "    cleaned_f1_scores = []\n",
        "    cleaned_reranker_scores = []\n",
        "\n",
        "    for item in cleaned_data:\n",
        "        original_pred = item['original_predict']\n",
        "        cleaned_pred = item['predict']\n",
        "        label = item['label']\n",
        "\n",
        "        # 原始分数\n",
        "        original_em_scores.append(exact_match(original_pred, label))\n",
        "        original_f1_scores.append(f1_score(original_pred, label))\n",
        "        original_reranker_scores.append(partial_match_score(original_pred, label, 0.5))\n",
        "\n",
        "        # 清理后分数\n",
        "        cleaned_em_scores.append(exact_match(cleaned_pred, label))\n",
        "        cleaned_f1_scores.append(f1_score(cleaned_pred, label))\n",
        "        cleaned_reranker_scores.append(partial_match_score(cleaned_pred, label, 0.5))\n",
        "\n",
        "    # 计算平均分数\n",
        "    original_results = {\n",
        "        'EM': np.mean(original_em_scores),\n",
        "        'F1': np.mean(original_f1_scores),\n",
        "        'Reranker': np.mean(original_reranker_scores)\n",
        "    }\n",
        "\n",
        "    cleaned_results = {\n",
        "        'EM': np.mean(cleaned_em_scores),\n",
        "        'F1': np.mean(cleaned_f1_scores),\n",
        "        'Reranker': np.mean(cleaned_reranker_scores)\n",
        "    }\n",
        "\n",
        "    # 打印对比结果\n",
        "    print(\"\\n=== 清理前后对比 ===\")\n",
        "    print(f\"样本数量: {len(data)}\")\n",
        "    print(f\"\\n{'指标':<15} {'原始':<10} {'清理后':<10} {'改进':<10}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for metric in ['EM', 'F1', 'Reranker']:\n",
        "        original = original_results[metric]\n",
        "        cleaned = cleaned_results[metric]\n",
        "        improvement = cleaned - original\n",
        "        print(f\"{metric:<15} {original:<10.4f} {cleaned:<10.4f} {improvement:>+.4f}\")\n",
        "\n",
        "    # 找出改进最明显的样本\n",
        "    print(\"\\n=== 改进最明显的样本 ===\")\n",
        "    improvements = []\n",
        "    for i, item in enumerate(cleaned_data):\n",
        "        original_f1 = f1_score(item['original_predict'], item['label'])\n",
        "        cleaned_f1 = f1_score(item['predict'], item['label'])\n",
        "        improvement = cleaned_f1 - original_f1\n",
        "        if improvement > 0.1:  # 改进超过0.1的样本\n",
        "            improvements.append((i, improvement, item))\n",
        "\n",
        "    improvements.sort(key=lambda x: x[1], reverse=True)\n",
        "    for i, (idx, improvement, item) in enumerate(improvements[:3]):\n",
        "        print(f\"\\n样本 {idx} (改进 +{improvement:.3f}):\")\n",
        "        print(f\"原始: {item['original_predict'][:100]}...\")\n",
        "        print(f\"清理: {item['predict'][:100]}...\")\n",
        "        print(f\"标签: {item['label'][:100]}...\")\n",
        "\n",
        "    # 保存清理后的数据\n",
        "    if output_file:\n",
        "        print(f\"\\n正在保存清理后的数据到: {output_file}\")\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            for item in cleaned_data:\n",
        "                # 只保存清理后的predict，移除original_predict以节省空间\n",
        "                save_item = {k: v for k, v in item.items() if k != 'original_predict'}\n",
        "                f.write(json.dumps(save_item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    return original_results, cleaned_results\n",
        "\n",
        "# 运行清理和评估\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/basevbshare/generated_predictions.jsonl\"\n",
        "    output_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/cleaned_predictions.jsonl\"\n",
        "\n",
        "    original_results, cleaned_results = clean_and_evaluate(input_file, output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v66oLarz0HKu"
      },
      "source": [
        "vb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Kpv9cUY0I7_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IXMD2gGvEaG"
      },
      "source": [
        "llm base\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIpIMRkdvBm_",
        "outputId": "66138bf7-0cdc-4282-a393-0b70762d944c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flagembedding\n",
            "  Downloading FlagEmbedding-1.3.5.tar.gz (163 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/163.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m163.8/163.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.9/163.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from flagembedding) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.44.2 in /usr/local/lib/python3.12/dist-packages (from flagembedding) (4.56.1)\n",
            "Requirement already satisfied: datasets>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from flagembedding) (4.0.0)\n",
            "Requirement already satisfied: accelerate>=0.20.1 in /usr/local/lib/python3.12/dist-packages (from flagembedding) (1.10.1)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.12/dist-packages (from flagembedding) (5.1.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from flagembedding) (0.17.1)\n",
            "Collecting ir-datasets (from flagembedding)\n",
            "  Downloading ir_datasets-0.5.11-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from flagembedding) (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from flagembedding) (5.29.5)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.20.1->flagembedding) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.20.1->flagembedding) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.20.1->flagembedding) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.20.1->flagembedding) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.20.1->flagembedding) (0.34.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.20.1->flagembedding) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.19.0->flagembedding) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.19.0->flagembedding) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.19.0->flagembedding) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.19.0->flagembedding) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.19.0->flagembedding) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.19.0->flagembedding) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.19.0->flagembedding) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.19.0->flagembedding) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->flagembedding) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->flagembedding) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.2->flagembedding) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.44.2->flagembedding) (0.22.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from ir-datasets->flagembedding) (4.13.5)\n",
            "Collecting inscriptis>=2.2.0 (from ir-datasets->flagembedding)\n",
            "  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.12/dist-packages (from ir-datasets->flagembedding) (5.4.0)\n",
            "Collecting trec-car-tools>=2.5.4 (from ir-datasets->flagembedding)\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
            "Collecting lz4>=3.1.10 (from ir-datasets->flagembedding)\n",
            "  Downloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting warc3-wet>=0.2.3 (from ir-datasets->flagembedding)\n",
            "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting warc3-wet-clueweb09>=0.2.5 (from ir-datasets->flagembedding)\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zlib-state>=0.1.3 (from ir-datasets->flagembedding)\n",
            "  Downloading zlib_state-0.1.10-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting ijson>=3.1.3 (from ir-datasets->flagembedding)\n",
            "  Downloading ijson-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting unlzw3>=0.2.1 (from ir-datasets->flagembedding)\n",
            "  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence_transformers->flagembedding) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence_transformers->flagembedding) (1.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence_transformers->flagembedding) (11.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets->flagembedding) (2.8)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->flagembedding) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.20.1->flagembedding) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.19.0->flagembedding) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.19.0->flagembedding) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.19.0->flagembedding) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.19.0->flagembedding) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.6.0->flagembedding) (1.3.0)\n",
            "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir-datasets->flagembedding)\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6.0->flagembedding) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.19.0->flagembedding) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.19.0->flagembedding) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.19.0->flagembedding) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers->flagembedding) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers->flagembedding) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->flagembedding) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->flagembedding) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->flagembedding) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->flagembedding) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->flagembedding) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->flagembedding) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->flagembedding) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.19.0->flagembedding) (1.17.0)\n",
            "Downloading ir_datasets-0.5.11-py3-none-any.whl (866 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.1/866.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ijson-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.3/148.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Downloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
            "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
            "Downloading zlib_state-0.1.10-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Building wheels for collected packages: flagembedding, warc3-wet-clueweb09, cbor\n",
            "  Building wheel for flagembedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flagembedding: filename=FlagEmbedding-1.3.5-py3-none-any.whl size=233746 sha256=116cf1a2ed3f93d74af9fd6f126f020c1c46a7d636b4279501d496e620055956\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/1f/f6/78f862bb80cb959cc9960b7c4e2d1f702b1bc0e79d19b5f124\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18919 sha256=e871b510d064719652810e02ade0bda9d19c659a3beab5064ad8710f2fd937a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/85/c2/9f0f621def52a1d5db7d29984f81e45f9fb6dfeb1a4eb6e31c\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp312-cp312-linux_x86_64.whl size=55021 sha256=662666d4e1a465a2266eb7df22ff61a321a39b1a3694689e06969c62f8d8a29c\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/3e/21/a739cbcc331a1ab45c326d6edbdac6118de4402f6076e30ff1\n",
            "Successfully built flagembedding warc3-wet-clueweb09 cbor\n",
            "Installing collected packages: warc3-wet-clueweb09, warc3-wet, cbor, zlib-state, unlzw3, trec-car-tools, lz4, ijson, inscriptis, ir-datasets, flagembedding\n",
            "Successfully installed cbor-1.0.0 flagembedding-1.3.5 ijson-3.4.0 inscriptis-2.6.0 ir-datasets-0.5.11 lz4-4.4.4 trec-car-tools-2.6 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.10\n"
          ]
        }
      ],
      "source": [
        "!pip install -U flagembedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atFzwPoo_oR6",
        "outputId": "f57e6ec2-3821-44d2-ea94-c47df0d9c4f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 直接GPT判断评估 ===\n",
            "输入文件: /content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/basevbshare/generated_predictions.jsonl\n",
            "加载了 548 个样本\n",
            "\n",
            "开始处理 548 个样本...\n",
            "处理样本 1/548 ✗ (The AI prediction does not acc...)\n",
            "处理样本 2/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 3/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 4/548 ✓\n",
            "处理样本 5/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 6/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 7/548 ✗ (The AI prediction provides det...)\n",
            "处理样本 8/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 9/548 ✗ (The AI prediction focuses sole...)\n",
            "处理样本 10/548 ✗ (The AI prediction is incoheren...)\n",
            "  进度: 10/548, 当前准确率: 0.100\n",
            "处理样本 11/548 ✗ (The AI prediction provides a J...)\n",
            "处理样本 12/548 ✗ (The AI prediction provides an ...)\n",
            "处理样本 13/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 14/548 ✗ (The AI prediction is unclear, ...)\n",
            "处理样本 15/548 ✗ (The AI prediction 'Toologicalv...)\n",
            "处理样本 16/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 17/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 18/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 19/548 ✗ (The AI prediction contains inc...)\n",
            "处理样本 20/548 ✗ (The AI prediction does not add...)\n",
            "  进度: 20/548, 当前准确率: 0.050\n",
            "处理样本 21/548 ✗ (The AI prediction is a jumbled...)\n",
            "处理样本 22/548 ✗ (The AI prediction is a jumbled...)\n",
            "处理样本 23/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 24/548 ✗ (The AI prediction contains inc...)\n",
            "处理样本 25/548 ✗ (The AI prediction contains irr...)\n",
            "处理样本 26/548 ✗ (The AI prediction inaccurately...)\n",
            "处理样本 27/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 28/548 ✗ (The AI prediction provides a c...)\n",
            "处理样本 29/548 ✓\n",
            "处理样本 30/548 ✗ (The AI prediction contains fac...)\n",
            "  进度: 30/548, 当前准确率: 0.067\n",
            "处理样本 31/548 ✗ (The AI prediction focuses on i...)\n",
            "处理样本 32/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 33/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 34/548 ✗ (The AI prediction does not ali...)\n",
            "处理样本 35/548 ✗ (The AI prediction is repetitiv...)\n",
            "处理样本 36/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 37/548 ✗ (The AI prediction focuses on a...)\n",
            "处理样本 38/548 ✓\n",
            "处理样本 39/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 40/548 ✗ (The AI prediction does not add...)\n",
            "  进度: 40/548, 当前准确率: 0.075\n",
            "处理样本 41/548 ✗ (The AI prediction provides an ...)\n",
            "处理样本 42/548 ✓\n",
            "处理样本 43/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 44/548 ✗ (The AI prediction provides a d...)\n",
            "处理样本 45/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 46/548 ✓\n",
            "处理样本 47/548 ✗ (The AI prediction provides a s...)\n",
            "处理样本 48/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 49/548 ✗ (The AI prediction is repetitiv...)\n",
            "处理样本 50/548 ✗ (The AI prediction is not relev...)\n",
            "  进度: 50/548, 当前准确率: 0.100\n",
            "处理样本 51/548 ✓\n",
            "处理样本 52/548 ✗ (The AI prediction is a long st...)\n",
            "处理样本 53/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 54/548 ✓\n",
            "处理样本 55/548 ✗ (The AI prediction does not con...)\n",
            "处理样本 56/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 57/548 ✗ (The AI prediction is incoheren...)\n",
            "处理样本 58/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 59/548 ✓\n",
            "处理样本 60/548 ✗ (The AI prediction does not pro...)\n",
            "  进度: 60/548, 当前准确率: 0.133\n",
            "处理样本 61/548 ✓\n",
            "处理样本 62/548 ✗ (The AI prediction contains mul...)\n",
            "处理样本 63/548 ✗ (The AI prediction is overly de...)\n",
            "处理样本 64/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 65/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 66/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 67/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 68/548 ✓\n",
            "处理样本 69/548 ✗ (The AI prediction does not acc...)\n",
            "处理样本 70/548 ✗ (The AI prediction describes ho...)\n",
            "  进度: 70/548, 当前准确率: 0.143\n",
            "处理样本 71/548 ✗ (The AI prediction contains ina...)\n",
            "处理样本 72/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 73/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 74/548 ✗ (The AI prediction discusses us...)\n",
            "处理样本 75/548 ✓\n",
            "处理样本 76/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 77/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 78/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 79/548 ✗ (The AI prediction is poorly st...)\n",
            "处理样本 80/548 ✗ (The AI prediction is poorly st...)\n",
            "  进度: 80/548, 当前准确率: 0.138\n",
            "处理样本 81/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 82/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 83/548 ✓\n",
            "处理样本 84/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 85/548 ✗ (The AI prediction is nonsensic...)\n",
            "处理样本 86/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 87/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 88/548 ✗ (The AI prediction contains rep...)\n",
            "处理样本 89/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 90/548 ✗ (The AI prediction does not inc...)\n",
            "  进度: 90/548, 当前准确率: 0.133\n",
            "处理样本 91/548 ✗ (The AI prediction discusses th...)\n",
            "处理样本 92/548 ✗ (The AI prediction does not ali...)\n",
            "处理样本 93/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 94/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 95/548 ✗ (The AI prediction is not relev...)\n",
            "处理样本 96/548 ✓\n",
            "处理样本 97/548 ✓\n",
            "处理样本 98/548 ✓\n",
            "处理样本 99/548 ✓\n",
            "处理样本 100/548 ✗ (The AI prediction incorrectly ...)\n",
            "  进度: 100/548, 当前准确率: 0.160\n",
            "处理样本 101/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 102/548 ✗ (The AI prediction is incoheren...)\n",
            "处理样本 103/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 104/548 ✗ (The AI prediction contains inc...)\n",
            "处理样本 105/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 106/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 107/548 ✗ (The AI prediction provides ste...)\n",
            "处理样本 108/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 109/548 ✗ (The AI prediction provides a s...)\n",
            "处理样本 110/548 ✗ (The AI prediction does not pro...)\n",
            "  进度: 110/548, 当前准确率: 0.145\n",
            "处理样本 111/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 112/548 ✗ (The AI prediction provides a g...)\n",
            "处理样本 113/548 ✓\n",
            "处理样本 114/548 ✗ (The AI prediction provides an ...)\n",
            "处理样本 115/548 ✗ (The AI prediction is a repetit...)\n",
            "处理样本 116/548 ✗ (The AI prediction does not ali...)\n",
            "处理样本 117/548 ✗ (The AI prediction provides a l...)\n",
            "处理样本 118/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 119/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 120/548 ✗ (The AI prediction is not factu...)\n",
            "  进度: 120/548, 当前准确率: 0.142\n",
            "处理样本 121/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 122/548 ✗ (The AI prediction is not factu...)\n",
            "处理样本 123/548 ✓\n",
            "处理样本 124/548 ✗ (The AI prediction provides a c...)\n",
            "处理样本 125/548 ✓\n",
            "处理样本 126/548 ✗ (The AI prediction contains mul...)\n",
            "处理样本 127/548 ✓\n",
            "处理样本 128/548 ✗ (The AI prediction is a long st...)\n",
            "处理样本 129/548 ✗ (The AI prediction is unclear a...)\n",
            "处理样本 130/548 ✗ (The AI prediction does not pro...)\n",
            "  进度: 130/548, 当前准确率: 0.154\n",
            "处理样本 131/548 ✗ (The AI prediction is unclear a...)\n",
            "处理样本 132/548 ✗ (The AI prediction provides a g...)\n",
            "处理样本 133/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 134/548 ✗ (The AI prediction contains inc...)\n",
            "处理样本 135/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 136/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 137/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 138/548 ✓\n",
            "处理样本 139/548 ✗ (The AI prediction is incomplet...)\n",
            "处理样本 140/548 ✗ (The AI prediction is incoheren...)\n",
            "  进度: 140/548, 当前准确率: 0.150\n",
            "处理样本 141/548 ✗ (The AI prediction does not acc...)\n",
            "处理样本 142/548 ✗ (The AI prediction is incoheren...)\n",
            "处理样本 143/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 144/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 145/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 146/548 ✗ (The AI prediction provides ste...)\n",
            "处理样本 147/548 ✗ (The AI prediction provides inc...)\n",
            "处理样本 148/548 ✗ (The AI prediction is incomplet...)\n",
            "处理样本 149/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 150/548 ✓\n",
            "  进度: 150/548, 当前准确率: 0.147\n",
            "处理样本 151/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 152/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 153/548 ✓\n",
            "处理样本 154/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 155/548 ✗ (The AI prediction is incoheren...)\n",
            "处理样本 156/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 157/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 158/548 ✗ (The AI prediction is unclear a...)\n",
            "处理样本 159/548 ✗ (The AI prediction discusses th...)\n",
            "处理样本 160/548 ✗ (The AI prediction is incoheren...)\n",
            "  进度: 160/548, 当前准确率: 0.144\n",
            "处理样本 161/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 162/548 ✗ (The AI prediction is unrelated...)\n",
            "处理样本 163/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 164/548 ✗ (The AI prediction is incomplet...)\n",
            "处理样本 165/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 166/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 167/548 ✗ (The AI prediction is a long st...)\n",
            "处理样本 168/548 ✗ (The AI prediction does not acc...)\n",
            "处理样本 169/548 ✓\n",
            "处理样本 170/548 ✗ (The AI prediction does not pro...)\n",
            "  进度: 170/548, 当前准确率: 0.141\n",
            "处理样本 171/548 ✗ (The AI prediction does not ans...)\n",
            "处理样本 172/548 ✗ (The AI prediction lacks clarit...)\n",
            "处理样本 173/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 174/548 ✗ (The AI prediction provides ste...)\n",
            "处理样本 175/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 176/548 ✗ (The AI prediction contains sev...)\n",
            "处理样本 177/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 178/548 ✗ (The AI prediction is not factu...)\n",
            "处理样本 179/548 ✗ (The AI prediction contains mul...)\n",
            "处理样本 180/548 ✗ (The AI prediction is unclear a...)\n",
            "  进度: 180/548, 当前准确率: 0.133\n",
            "处理样本 181/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 182/548 ✗ (The AI prediction is unclear a...)\n",
            "处理样本 183/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 184/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 185/548 ✗ (The AI prediction focuses on r...)\n",
            "处理样本 186/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 187/548 ✓\n",
            "处理样本 188/548 ✗ (The AI prediction does not acc...)\n",
            "处理样本 189/548 ✗ (The AI prediction provides a l...)\n",
            "处理样本 190/548 ✗ (The AI prediction provides a p...)\n",
            "  进度: 190/548, 当前准确率: 0.132\n",
            "处理样本 191/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 192/548 ✓\n",
            "处理样本 193/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 194/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 195/548 ✓\n",
            "处理样本 196/548 ✗ (The AI prediction provides a g...)\n",
            "处理样本 197/548 ✗ (The AI prediction contains mul...)\n",
            "处理样本 198/548 ✗ (The AI prediction is repetitiv...)\n",
            "处理样本 199/548 ✗ (The AI prediction provides a l...)\n",
            "处理样本 200/548 ✗ (The AI prediction does not pro...)\n",
            "  进度: 200/548, 当前准确率: 0.135\n",
            "处理样本 201/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 202/548 ✗ (The AI prediction does not men...)\n",
            "处理样本 203/548 ✗ (The AI prediction is unclear a...)\n",
            "处理样本 204/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 205/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 206/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 207/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 208/548 ✗ (The AI prediction contains mul...)\n",
            "处理样本 209/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 210/548 ✗ (The AI prediction is incomplet...)\n",
            "  进度: 210/548, 当前准确率: 0.129\n",
            "处理样本 211/548 ✗ (The AI prediction focuses on g...)\n",
            "处理样本 212/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 213/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 214/548 ✗ (The AI prediction is not relev...)\n",
            "处理样本 215/548 ✗ (The AI prediction is unclear a...)\n",
            "处理样本 216/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 217/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 218/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 219/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 220/548 ✗ (The AI prediction is unclear a...)\n",
            "  进度: 220/548, 当前准确率: 0.123\n",
            "处理样本 221/548 ✗ (The AI prediction provides a c...)\n",
            "处理样本 222/548 ✓\n",
            "处理样本 223/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 224/548 ✗ (The AI prediction provides gen...)\n",
            "处理样本 225/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 226/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 227/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 228/548 ✓\n",
            "处理样本 229/548 ✓\n",
            "处理样本 230/548 ✓\n",
            "  进度: 230/548, 当前准确率: 0.135\n",
            "处理样本 231/548 ✗ (The AI prediction lists multip...)\n",
            "处理样本 232/548 ✓\n",
            "处理样本 233/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 234/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 235/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 236/548 ✗ (The AI prediction is unclear, ...)\n",
            "处理样本 237/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 238/548 ✗ (The AI prediction contains mul...)\n",
            "处理样本 239/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 240/548 ✗ (The AI prediction contains num...)\n",
            "  进度: 240/548, 当前准确率: 0.133\n",
            "处理样本 241/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 242/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 243/548 ✗ (The AI prediction is unclear, ...)\n",
            "处理样本 244/548 ✓\n",
            "处理样本 245/548 ✗ (The AI prediction is a long st...)\n",
            "处理样本 246/548 ✗ (The AI prediction contains sev...)\n",
            "处理样本 247/548 ✗ (The AI prediction lacks the de...)\n",
            "处理样本 248/548 ✓\n",
            "处理样本 249/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 250/548 ✓\n",
            "  进度: 250/548, 当前准确率: 0.140\n",
            "处理样本 251/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 252/548 ✗ (The AI prediction is fragmente...)\n",
            "处理样本 253/548 ✓\n",
            "处理样本 254/548 ✗ (The AI prediction is overly re...)\n",
            "处理样本 255/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 256/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 257/548 ✗ (The AI prediction suggests imp...)\n",
            "处理样本 258/548 ✗ (The AI prediction is unclear a...)\n",
            "处理样本 259/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 260/548 ✓\n",
            "  进度: 260/548, 当前准确率: 0.142\n",
            "处理样本 261/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 262/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 263/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 264/548 ✓\n",
            "处理样本 265/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 266/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 267/548 ✗ (The AI prediction focuses on a...)\n",
            "处理样本 268/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 269/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 270/548 ✗ (The AI prediction provides a r...)\n",
            "  进度: 270/548, 当前准确率: 0.141\n",
            "处理样本 271/548 ✗ (The AI prediction lists variou...)\n",
            "处理样本 272/548 ✗ (The AI prediction provides a l...)\n",
            "处理样本 273/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 274/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 275/548 ✗ (The AI prediction is a fragmen...)\n",
            "处理样本 276/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 277/548 ✗ (The AI prediction is not factu...)\n",
            "处理样本 278/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 279/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 280/548 ✗ (The AI prediction does not add...)\n",
            "  进度: 280/548, 当前准确率: 0.136\n",
            "处理样本 281/548 ✗ (The AI prediction focuses sole...)\n",
            "处理样本 282/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 283/548 ✓\n",
            "处理样本 284/548 ✗ (The AI prediction is unclear a...)\n",
            "处理样本 285/548 ✗ (The AI prediction describes Ja...)\n",
            "处理样本 286/548 ✗ (The AI prediction is fragmente...)\n",
            "处理样本 287/548 ✗ (The AI prediction provides inc...)\n",
            "处理样本 288/548 ✗ (The AI prediction contains sev...)\n",
            "处理样本 289/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 290/548 ✗ (The AI prediction is unclear a...)\n",
            "  进度: 290/548, 当前准确率: 0.134\n",
            "处理样本 291/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 292/548 ✗ (The AI prediction is vague and...)\n",
            "处理样本 293/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 294/548 ✗ (The AI prediction provides ins...)\n",
            "处理样本 295/548 ✗ (The AI prediction focuses on u...)\n",
            "处理样本 296/548 ✗ (The AI prediction provides a r...)\n",
            "处理样本 297/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 298/548 ✓\n",
            "处理样本 299/548 ✓\n",
            "处理样本 300/548 ✗ (The AI prediction incorrectly ...)\n",
            "  进度: 300/548, 当前准确率: 0.137\n",
            "处理样本 301/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 302/548 ✗ (The AI prediction is nonsensic...)\n",
            "处理样本 303/548 ✓\n",
            "处理样本 304/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 305/548 ✗ (The AI prediction is disjointe...)\n",
            "处理样本 306/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 307/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 308/548 ✗ (The AI prediction focuses on r...)\n",
            "处理样本 309/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 310/548 ✗ (The AI prediction provides a l...)\n",
            "  进度: 310/548, 当前准确率: 0.135\n",
            "处理样本 311/548 ✓\n",
            "处理样本 312/548 ✗ (The AI prediction contains sev...)\n",
            "处理样本 313/548 ✗ (The AI prediction describes a ...)\n",
            "处理样本 314/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 315/548 ✗ (The AI prediction does not acc...)\n",
            "处理样本 316/548 ✗ (The AI prediction provides a s...)\n",
            "处理样本 317/548 ✗ (The AI prediction inaccurately...)\n",
            "处理样本 318/548 ✓\n",
            "处理样本 319/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 320/548 ✗ (The AI prediction is not factu...)\n",
            "  进度: 320/548, 当前准确率: 0.138\n",
            "处理样本 321/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 322/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 323/548 ✗ (The AI prediction provides a C...)\n",
            "处理样本 324/548 ✗ (The AI prediction only demonst...)\n",
            "处理样本 325/548 ✗ (The AI prediction is fragmente...)\n",
            "处理样本 326/548 ✗ (The AI prediction is repetitiv...)\n",
            "处理样本 327/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 328/548 ✓\n",
            "处理样本 329/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 330/548 ✗ (The AI prediction contains inc...)\n",
            "  进度: 330/548, 当前准确率: 0.136\n",
            "处理样本 331/548 ✗ (The AI prediction is unclear a...)\n",
            "处理样本 332/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 333/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 334/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 335/548 ✗ (The AI prediction misrepresent...)\n",
            "处理样本 336/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 337/548 ✗ (The AI prediction lists studie...)\n",
            "处理样本 338/548 ✗ (The AI prediction contains non...)\n",
            "处理样本 339/548 ✗ (The AI prediction is incoheren...)\n",
            "处理样本 340/548 ✗ (The AI prediction incorrectly ...)\n",
            "  进度: 340/548, 当前准确率: 0.132\n",
            "处理样本 341/548 ✗ (The AI prediction discusses so...)\n",
            "处理样本 342/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 343/548 ✓\n",
            "处理样本 344/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 345/548 ✓\n",
            "处理样本 346/548 ✗ (The AI prediction is a long st...)\n",
            "处理样本 347/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 348/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 349/548 ✓\n",
            "处理样本 350/548 ✗ (The AI prediction does not ali...)\n",
            "  进度: 350/548, 当前准确率: 0.137\n",
            "处理样本 351/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 352/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 353/548 ✗ (The AI prediction is incoheren...)\n",
            "处理样本 354/548 ✗ (The AI prediction does not acc...)\n",
            "处理样本 355/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 356/548 ✗ (The AI prediction is incomplet...)\n",
            "处理样本 357/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 358/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 359/548 ✗ (The AI prediction provides a s...)\n",
            "处理样本 360/548 ✗ (The AI prediction lists variou...)\n",
            "  进度: 360/548, 当前准确率: 0.133\n",
            "处理样本 361/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 362/548 ✓\n",
            "处理样本 363/548 ✗ (The AI prediction does not acc...)\n",
            "处理样本 364/548 ✓\n",
            "处理样本 365/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 366/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 367/548 ✗ (The AI prediction provides a l...)\n",
            "处理样本 368/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 369/548 ✓\n",
            "处理样本 370/548 ✗ (The AI prediction is repetitiv...)\n",
            "  进度: 370/548, 当前准确率: 0.138\n",
            "处理样本 371/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 372/548 ✗ (The AI prediction provides alt...)\n",
            "处理样本 373/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 374/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 375/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 376/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 377/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 378/548 ✗ (The AI prediction does not acc...)\n",
            "处理样本 379/548 ✗ (The AI prediction is unclear a...)\n",
            "处理样本 380/548 ✗ (The AI prediction contains inc...)\n",
            "  进度: 380/548, 当前准确率: 0.134\n",
            "处理样本 381/548 ✗ (The AI prediction contains rep...)\n",
            "处理样本 382/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 383/548 ✗ (The AI prediction is unclear a...)\n",
            "处理样本 384/548 ✗ (The AI prediction contains inc...)\n",
            "处理样本 385/548 ✗ (The AI prediction does not imp...)\n",
            "处理样本 386/548 ✗ (The AI prediction contains inc...)\n",
            "处理样本 387/548 ✗ (The AI prediction does not acc...)\n",
            "处理样本 388/548 ✗ (The AI prediction does not ali...)\n",
            "处理样本 389/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 390/548 ✗ (The AI prediction is poorly st...)\n",
            "  进度: 390/548, 当前准确率: 0.131\n",
            "处理样本 391/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 392/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 393/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 394/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 395/548 ✓\n",
            "处理样本 396/548 ✗ (The AI prediction focuses prim...)\n",
            "处理样本 397/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 398/548 ✗ (The AI prediction contains sev...)\n",
            "处理样本 399/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 400/548 ✗ (The AI prediction contains num...)\n",
            "  进度: 400/548, 当前准确率: 0.130\n",
            "处理样本 401/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 402/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 403/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 404/548 ✗ (The AI prediction is repetitiv...)\n",
            "处理样本 405/548 ✓\n",
            "处理样本 406/548 ✗ (The AI prediction is unclear a...)\n",
            "处理样本 407/548 ✗ (The AI prediction is repetitiv...)\n",
            "处理样本 408/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 409/548 ✓\n",
            "处理样本 410/548 ✗ (The AI prediction is largely i...)\n",
            "  进度: 410/548, 当前准确率: 0.132\n",
            "处理样本 411/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 412/548 ✓\n",
            "处理样本 413/548 ✗ (The AI prediction is incoheren...)\n",
            "处理样本 414/548 ✗ (The AI prediction is fragmente...)\n",
            "处理样本 415/548 ✗ (The AI prediction is fragmente...)\n",
            "处理样本 416/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 417/548 ✗ (The AI prediction does not ali...)\n",
            "处理样本 418/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 419/548 ✗ (The AI prediction is incoheren...)\n",
            "处理样本 420/548 ✗ (The AI prediction does not acc...)\n",
            "  进度: 420/548, 当前准确率: 0.131\n",
            "处理样本 421/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 422/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 423/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 424/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 425/548 ✗ (The AI prediction discusses th...)\n",
            "处理样本 426/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 427/548 ✗ (The AI prediction does not acc...)\n",
            "处理样本 428/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 429/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 430/548 ✗ (The AI prediction does not pro...)\n",
            "  进度: 430/548, 当前准确率: 0.128\n",
            "处理样本 431/548 ✗ (The AI prediction describes a ...)\n",
            "处理样本 432/548 ✗ (The AI prediction provides err...)\n",
            "处理样本 433/548 ✗ (The AI prediction is incomplet...)\n",
            "处理样本 434/548 ✗ (The AI prediction contains a s...)\n",
            "处理样本 435/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 436/548 ✗ (The AI prediction identifies a...)\n",
            "处理样本 437/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 438/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 439/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 440/548 ✗ (The AI prediction does not add...)\n",
            "  进度: 440/548, 当前准确率: 0.125\n",
            "处理样本 441/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 442/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 443/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 444/548 ✓\n",
            "处理样本 445/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 446/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 447/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 448/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 449/548 ✓\n",
            "处理样本 450/548 ✗ (The AI prediction does not pro...)\n",
            "  进度: 450/548, 当前准确率: 0.127\n",
            "处理样本 451/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 452/548 ✗ (The AI prediction consists of ...)\n",
            "处理样本 453/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 454/548 ✗ (The AI prediction does not con...)\n",
            "处理样本 455/548 ✓\n",
            "处理样本 456/548 ✗ (The AI prediction provides an ...)\n",
            "处理样本 457/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 458/548 ✗ (The AI prediction is unclear a...)\n",
            "处理样本 459/548 ✗ (The AI prediction is incoheren...)\n",
            "处理样本 460/548 ✗ (The AI prediction is excessive...)\n",
            "  进度: 460/548, 当前准确率: 0.126\n",
            "处理样本 461/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 462/548 ✗ (The AI prediction is incoheren...)\n",
            "处理样本 463/548 ✗ (The AI prediction is overly re...)\n",
            "处理样本 464/548 ✗ (The AI prediction is incoheren...)\n",
            "处理样本 465/548 ✗ (The AI prediction is incoheren...)\n",
            "处理样本 466/548 ✗ (The AI prediction is nonsensic...)\n",
            "处理样本 467/548 ✓\n",
            "处理样本 468/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 469/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 470/548 ✗ (The AI prediction does not pro...)\n",
            "  进度: 470/548, 当前准确率: 0.126\n",
            "处理样本 471/548 ✗ (The AI prediction lists only o...)\n",
            "处理样本 472/548 ✓\n",
            "处理样本 473/548 ✗ (The AI prediction does not ali...)\n",
            "处理样本 474/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 475/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 476/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 477/548 ✓\n",
            "处理样本 478/548 ✗ (The AI prediction is repetitiv...)\n",
            "处理样本 479/548 ✗ (The AI prediction provides an ...)\n",
            "处理样本 480/548 ✗ (The AI prediction does not pro...)\n",
            "  进度: 480/548, 当前准确率: 0.127\n",
            "处理样本 481/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 482/548 ✗ (The AI prediction is repetitiv...)\n",
            "处理样本 483/548 ✗ (The AI prediction provides inc...)\n",
            "处理样本 484/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 485/548 ✓\n",
            "处理样本 486/548 ✓\n",
            "处理样本 487/548 ✗ (The AI prediction is unclear, ...)\n",
            "处理样本 488/548 ✗ (The AI prediction inaccurately...)\n",
            "处理样本 489/548 ✗ (The AI prediction discusses th...)\n",
            "处理样本 490/548 ✗ (The AI prediction does not pro...)\n",
            "  进度: 490/548, 当前准确率: 0.129\n",
            "处理样本 491/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 492/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 493/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 494/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 495/548 ✗ (The AI prediction is incoheren...)\n",
            "处理样本 496/548 ✗ (The AI prediction is incoheren...)\n",
            "处理样本 497/548 ✓\n",
            "处理样本 498/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 499/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 500/548 ✗ (The AI prediction contains num...)\n",
            "  进度: 500/548, 当前准确率: 0.128\n",
            "处理样本 501/548 ✗ (The AI prediction is nonsensic...)\n",
            "处理样本 502/548 ✗ (The AI prediction focuses on t...)\n",
            "处理样本 503/548 ✗ (The AI prediction is unclear a...)\n",
            "处理样本 504/548 ✗ (The AI prediction is repetitiv...)\n",
            "处理样本 505/548 ✗ (The AI prediction provides an ...)\n",
            "处理样本 506/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 507/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 508/548 ✗ (The AI prediction provides gen...)\n",
            "处理样本 509/548 ✓\n",
            "处理样本 510/548 ✗ (The AI prediction does not pro...)\n",
            "  进度: 510/548, 当前准确率: 0.127\n",
            "处理样本 511/548 ✓\n",
            "处理样本 512/548 ✓\n",
            "处理样本 513/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 514/548 ✗ (The AI prediction discusses th...)\n",
            "处理样本 515/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 516/548 ✗ (The AI prediction is unclear a...)\n",
            "处理样本 517/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 518/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 519/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 520/548 ✗ (The AI prediction provides a r...)\n",
            "  进度: 520/548, 当前准确率: 0.129\n",
            "处理样本 521/548 ✗ (The AI prediction does not imp...)\n",
            "处理样本 522/548 ✗ (The AI prediction is missing t...)\n",
            "处理样本 523/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 524/548 ✗ (The AI prediction contains sev...)\n",
            "处理样本 525/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 526/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 527/548 ✗ (The AI prediction does not acc...)\n",
            "处理样本 528/548 ✗ (The AI prediction discusses th...)\n",
            "处理样本 529/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 530/548 ✓\n",
            "  进度: 530/548, 当前准确率: 0.128\n",
            "处理样本 531/548 ✓\n",
            "处理样本 532/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 533/548 ✗ (The AI prediction is largely i...)\n",
            "处理样本 534/548 ✓\n",
            "处理样本 535/548 ✓\n",
            "处理样本 536/548 ✓\n",
            "处理样本 537/548 ✗ (The AI prediction does not cor...)\n",
            "处理样本 538/548 ✗ (The AI prediction does not add...)\n",
            "处理样本 539/548 ✗ (The AI prediction contains sev...)\n",
            "处理样本 540/548 ✗ (The AI prediction is unclear a...)\n",
            "  进度: 540/548, 当前准确率: 0.133\n",
            "处理样本 541/548 ✗ (The AI prediction contains num...)\n",
            "处理样本 542/548 ✗ (The AI prediction lacks struct...)\n",
            "处理样本 543/548 ✓\n",
            "处理样本 544/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 545/548 ✗ (The AI prediction is repetitiv...)\n",
            "处理样本 546/548 ✗ (The AI prediction contains ina...)\n",
            "处理样本 547/548 ✗ (The AI prediction does not pro...)\n",
            "处理样本 548/548 ✗ (The AI prediction does not pro...)\n",
            "\n",
            "=== 评估结果 ===\n",
            "总样本数: 548\n",
            "正确样本: 73\n",
            "错误样本: 475\n",
            "API错误: 0\n",
            "最终准确率: 0.1332 (13.32%)\n",
            "\n",
            "=== 正确样本示例 ===\n",
            "预测: The original paragraph is:\n",
            "\n",
            "By the grace of the gods, the arcane and enigmatic art of metaphorical language has been summoned forth to shed light upon the bewildering addressing modes of the instructi...\n",
            "参考: Lo and behold! By the grace of divine intervention, the incomprehensible and enigmatic art of metaphorical language has been invoked to elucidate the inscrutable addressing modes of the instructions t...\n",
            "原因: The AI prediction conveys the same core message and information as the reference answer, discussing the metaphorical language used to describe addressing modes and listing the same commands. Although there are differences in wording and phrasing, the essential meaning and intent remain intact.\n",
            "\n",
            "预测: True\n",
            "\n",
            "参考: True. The extension of the thumb occurs at right angles (90 degrees) to the plane of the palm. This movement is also known as extension in the sagittal plane, and it allows the thumb to move away from...\n",
            "原因: The AI prediction 'True' is factually correct as it confirms the statement about thumb extension. The reference provides additional context about the movement, which is helpful but not necessary for the correctness of the prediction.\n",
            "\n",
            "=== 错误样本示例 ===\n",
            "预测: Tony the expertise and providing credibility.\n",
            " the audience.\n",
            "5. Call: The presenter offers the offer or service they they are are, and why why in solves a problem or and how value proof of to.\n",
            " claims...\n",
            "参考: Tony Robbins describes six core human needs that drive our behaviors and motivations. These six needs are:\n",
            "\n",
            "1. Certainty: The need for safety, stability, and predictability. This includes the need for...\n",
            "原因: The AI prediction does not accurately address the core human needs described by Tony Robbins. Instead, it appears to be a disorganized and incomplete response that lacks clarity and coherence, failing to convey the essential information about the six core human needs.\n",
            "\n",
            "预测: Sure, I can tell you how to tell if a customer segment is well segmented. Here are three bullet points to help you out:\n",
            "\n",
            "1. Does the customer segment have a clear and distinct identity?\n",
            "2. Are the cus...\n",
            "参考: 1. Homogeneity: The segment should consist of customers who share similar characteristics and behaviors.\n",
            "2. Distinctiveness: The segment should be different from other segments in terms of their chara...\n",
            "原因: The AI prediction does not provide accurate or helpful criteria for determining if a customer segment is well segmented. The points mentioned are repetitive and lack the essential aspects of homogeneity, distinctiveness, and stability that are outlined in the reference answer.\n",
            "\n",
            "\n",
            "结果已保存到: gpt_direct_evaluation_results.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# 设置API Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "client = OpenAI()\n",
        "\n",
        "def read_jsonl_data(path):\n",
        "    \"\"\"读取JSONL数据\"\"\"\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                item = json.loads(line)\n",
        "                data.append({\n",
        "                    'prompt': item.get('prompt', ''),\n",
        "                    'prediction': item.get('predict', ''),\n",
        "                    'reference': item.get('label', '')\n",
        "                })\n",
        "    return data\n",
        "\n",
        "def gpt_judge(prediction, reference, prompt=\"\"):\n",
        "    \"\"\"直接用GPT判断\"\"\"\n",
        "\n",
        "    # 识别任务类型\n",
        "    task_type = \"general conversation\"\n",
        "    if prompt:\n",
        "        prompt_lower = prompt.lower()\n",
        "        if any(word in prompt_lower for word in ['code', 'programming', 'function', 'algorithm']):\n",
        "            task_type = \"coding\"\n",
        "        elif any(word in prompt_lower for word in ['explain', 'what is', 'describe', 'definition']):\n",
        "            task_type = \"explanation\"\n",
        "        elif any(word in prompt_lower for word in ['list', 'steps', 'how to', 'instruction']):\n",
        "            task_type = \"instruction\"\n",
        "        elif any(word in prompt_lower for word in ['analyze', 'compare', 'evaluate']):\n",
        "            task_type = \"analysis\"\n",
        "\n",
        "    judge_prompt = f\"\"\"You are evaluating an AI assistant's response for a {task_type} task.\n",
        "\n",
        "Evaluation Guidelines:\n",
        "- Focus on factual correctness and helpfulness\n",
        "- Accept different but semantically equivalent answers\n",
        "- Ignore formatting, style, and minor wording differences\n",
        "- Consider the core message and information accuracy\n",
        "- Be reasonably lenient - if the prediction conveys the same key information as the reference, it should be considered correct\n",
        "- For explanations: focus on conceptual accuracy rather than exact phrasing\n",
        "- For instructions: check if the steps achieve the same goal\n",
        "- For analysis: evaluate logical reasoning and key conclusions\n",
        "\n",
        "Important: Many responses may be worded differently but still be factually correct and helpful.\n",
        "\n",
        "Output format: {{\"score\": 1, \"reason\": \"explanation\"}} for correct responses\n",
        "               {{\"score\": 0, \"reason\": \"explanation\"}} for incorrect responses\n",
        "\n",
        "Score 1: The prediction is factually correct and helpful (even if expressed differently)\n",
        "Score 0: The prediction is factually wrong, misleading, or significantly unhelpful\"\"\"\n",
        "\n",
        "    # 限制文本长度以节省token\n",
        "    max_length = 1000\n",
        "    pred_text = prediction[:max_length]\n",
        "    ref_text = reference[:max_length]\n",
        "\n",
        "    # 添加省略号如果被截断\n",
        "    if len(prediction) > max_length:\n",
        "        pred_text += \" [truncated...]\"\n",
        "    if len(reference) > max_length:\n",
        "        ref_text += \" [truncated...]\"\n",
        "\n",
        "    user_message = f\"\"\"Task Type: {task_type}\n",
        "\n",
        "AI Prediction:\n",
        "{pred_text}\n",
        "\n",
        "Reference Answer:\n",
        "{ref_text}\n",
        "\n",
        "Is the AI prediction factually correct and helpful compared to the reference? Output JSON only.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0.1,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": judge_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ],\n",
        "            max_tokens=150\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        # 解析JSON\n",
        "        try:\n",
        "            # 查找JSON部分\n",
        "            start = content.find(\"{\")\n",
        "            end = content.rfind(\"}\") + 1\n",
        "\n",
        "            if start != -1 and end > start:\n",
        "                json_text = content[start:end]\n",
        "                result = json.loads(json_text)\n",
        "\n",
        "                score = int(result.get(\"score\", 0))\n",
        "                reason = result.get(\"reason\", \"no reason provided\")\n",
        "\n",
        "                return score, reason\n",
        "            else:\n",
        "                # 如果没有找到JSON，尝试从文本中提取\n",
        "                if \"score\" in content.lower():\n",
        "                    if \"1\" in content and (\"correct\" in content.lower() or \"accurate\" in content.lower()):\n",
        "                        return 1, \"extracted_positive\"\n",
        "                    else:\n",
        "                        return 0, \"extracted_negative\"\n",
        "                return 0, f\"parse_failed: {content}\"\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            # JSON解析失败，基于关键词判断\n",
        "            content_lower = content.lower()\n",
        "            if any(word in content_lower for word in [\"correct\", \"accurate\", \"helpful\", \"good\"]):\n",
        "                return 1, f\"keyword_positive: {content[:50]}\"\n",
        "            else:\n",
        "                return 0, f\"keyword_negative: {content[:50]}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return 0, f\"api_error: {str(e)}\"\n",
        "\n",
        "def evaluate_direct_gpt(input_path, sample_size=None, save_results=True):\n",
        "    \"\"\"直接用GPT评估所有样本\"\"\"\n",
        "\n",
        "    print(\"=== 直接GPT判断评估 ===\")\n",
        "    print(f\"输入文件: {input_path}\")\n",
        "\n",
        "    # 读取数据\n",
        "    data = read_jsonl_data(input_path)\n",
        "    print(f\"加载了 {len(data)} 个样本\")\n",
        "\n",
        "    # 采样\n",
        "    if sample_size and sample_size < len(data):\n",
        "        import random\n",
        "        data = random.sample(data, sample_size)\n",
        "        print(f\"采样 {sample_size} 个样本进行评估\")\n",
        "\n",
        "    results = []\n",
        "    correct_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    print(f\"\\n开始处理 {len(data)} 个样本...\")\n",
        "\n",
        "    for i, item in enumerate(data):\n",
        "        print(f\"处理样本 {i+1}/{len(data)}\", end=\"\")\n",
        "\n",
        "        pred = item['prediction']\n",
        "        ref = item['reference']\n",
        "        prompt = item['prompt']\n",
        "\n",
        "        # 直接调用GPT判断\n",
        "        score, reason = gpt_judge(pred, ref, prompt)\n",
        "\n",
        "        if score == 1:\n",
        "            correct_count += 1\n",
        "            print(\" ✓\")\n",
        "        else:\n",
        "            print(f\" ✗ ({reason[:30]}...)\")\n",
        "\n",
        "        if \"api_error\" in reason:\n",
        "            error_count += 1\n",
        "\n",
        "        results.append({\n",
        "            'index': i,\n",
        "            'prompt': prompt[:100] + \"...\" if len(prompt) > 100 else prompt,\n",
        "            'prediction': pred[:200] + \"...\" if len(pred) > 200 else pred,\n",
        "            'reference': ref[:200] + \"...\" if len(ref) > 200 else ref,\n",
        "            'gpt_score': score,\n",
        "            'gpt_reason': reason\n",
        "        })\n",
        "\n",
        "        # API限速\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        # 每10个样本显示进度\n",
        "        if (i + 1) % 10 == 0:\n",
        "            current_accuracy = correct_count / (i + 1)\n",
        "            print(f\"  进度: {i+1}/{len(data)}, 当前准确率: {current_accuracy:.3f}\")\n",
        "\n",
        "    # 计算最终结果\n",
        "    total_samples = len(results)\n",
        "    accuracy = correct_count / total_samples\n",
        "\n",
        "    print(f\"\\n=== 评估结果 ===\")\n",
        "    print(f\"总样本数: {total_samples}\")\n",
        "    print(f\"正确样本: {correct_count}\")\n",
        "    print(f\"错误样本: {total_samples - correct_count}\")\n",
        "    print(f\"API错误: {error_count}\")\n",
        "    print(f\"最终准确率: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "    # 显示一些示例\n",
        "    print(f\"\\n=== 正确样本示例 ===\")\n",
        "    correct_examples = [r for r in results if r['gpt_score'] == 1][:2]\n",
        "    for ex in correct_examples:\n",
        "        print(f\"预测: {ex['prediction']}\")\n",
        "        print(f\"参考: {ex['reference']}\")\n",
        "        print(f\"原因: {ex['gpt_reason']}\")\n",
        "        print()\n",
        "\n",
        "    print(f\"=== 错误样本示例 ===\")\n",
        "    incorrect_examples = [r for r in results if r['gpt_score'] == 0][:2]\n",
        "    for ex in incorrect_examples:\n",
        "        print(f\"预测: {ex['prediction']}\")\n",
        "        print(f\"参考: {ex['reference']}\")\n",
        "        print(f\"原因: {ex['gpt_reason']}\")\n",
        "        print()\n",
        "\n",
        "    # 保存结果\n",
        "    if save_results:\n",
        "        output_file = f\"gpt_direct_evaluation_results.json\"\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({\n",
        "                'summary': {\n",
        "                    'total_samples': total_samples,\n",
        "                    'correct_count': correct_count,\n",
        "                    'accuracy': accuracy,\n",
        "                    'api_errors': error_count\n",
        "                },\n",
        "                'detailed_results': results\n",
        "            }, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\\n结果已保存到: {output_file}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def quick_test(input_path, n_samples=3):\n",
        "    \"\"\"快速测试GPT调用\"\"\"\n",
        "    print(\"=== 快速GPT测试 ===\")\n",
        "\n",
        "    data = read_jsonl_data(input_path)\n",
        "    if not data:\n",
        "        print(\"无法读取数据\")\n",
        "        return\n",
        "\n",
        "    # 测试前几个样本\n",
        "    for i in range(min(n_samples, len(data))):\n",
        "        item = data[i]\n",
        "        print(f\"\\n--- 测试样本 {i+1} ---\")\n",
        "        print(f\"预测: {item['prediction'][:100]}...\")\n",
        "        print(f\"参考: {item['reference'][:100]}...\")\n",
        "\n",
        "        score, reason = gpt_judge(item['prediction'], item['reference'], item['prompt'])\n",
        "\n",
        "        print(f\"GPT评分: {score}\")\n",
        "        print(f\"原因: {reason}\")\n",
        "\n",
        "        time.sleep(1)  # 短暂暂停\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/basevbshare/generated_predictions.jsonl\"\n",
        "\n",
        "    results = evaluate_direct_gpt(input_file, sample_size=None, save_results=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqJEzueIwxA3"
      },
      "source": [
        "lora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "WbFWCE4E1QtL",
        "outputId": "a1d3240e-2070-46e0-8856-e51ccb3d3919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 直接GPT判断评估 ===\n",
            "输入文件: /content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/sharegpttest/generated_predictions.jsonl\n",
            "加载了 548 个样本\n",
            "\n",
            "开始处理 548 个样本...\n",
            "处理样本 1/548 ✗ (The AI prediction incorrectly ...)\n",
            "处理样本 2/548"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3299882625.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/sharegpttest/generated_predictions.jsonl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_direct_gpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3299882625.py\u001b[0m in \u001b[0;36mevaluate_direct_gpt\u001b[0;34m(input_path, sample_size, save_results)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# 直接调用GPT判断\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt_judge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3299882625.py\u001b[0m in \u001b[0;36mgpt_judge\u001b[0;34m(prediction, reference, prompt)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o-mini\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m    983\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1230\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# 设置API Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "client = OpenAI()\n",
        "\n",
        "def read_jsonl_data(path):\n",
        "    \"\"\"读取JSONL数据\"\"\"\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                item = json.loads(line)\n",
        "                data.append({\n",
        "                    'prompt': item.get('prompt', ''),\n",
        "                    'prediction': item.get('predict', ''),\n",
        "                    'reference': item.get('label', '')\n",
        "                })\n",
        "    return data\n",
        "\n",
        "def gpt_judge(prediction, reference, prompt=\"\"):\n",
        "    \"\"\"直接用GPT判断\"\"\"\n",
        "\n",
        "    # 识别任务类型\n",
        "    task_type = \"general conversation\"\n",
        "    if prompt:\n",
        "        prompt_lower = prompt.lower()\n",
        "        if any(word in prompt_lower for word in ['code', 'programming', 'function', 'algorithm']):\n",
        "            task_type = \"coding\"\n",
        "        elif any(word in prompt_lower for word in ['explain', 'what is', 'describe', 'definition']):\n",
        "            task_type = \"explanation\"\n",
        "        elif any(word in prompt_lower for word in ['list', 'steps', 'how to', 'instruction']):\n",
        "            task_type = \"instruction\"\n",
        "        elif any(word in prompt_lower for word in ['analyze', 'compare', 'evaluate']):\n",
        "            task_type = \"analysis\"\n",
        "\n",
        "    judge_prompt = f\"\"\"You are evaluating an AI assistant's response for a {task_type} task.\n",
        "\n",
        "Evaluation Guidelines:\n",
        "- Focus on factual correctness and helpfulness\n",
        "- Accept different but semantically equivalent answers\n",
        "- Ignore formatting, style, and minor wording differences\n",
        "- Consider the core message and information accuracy\n",
        "- Be reasonably lenient - if the prediction conveys the same key information as the reference, it should be considered correct\n",
        "- For explanations: focus on conceptual accuracy rather than exact phrasing\n",
        "- For instructions: check if the steps achieve the same goal\n",
        "- For analysis: evaluate logical reasoning and key conclusions\n",
        "\n",
        "Important: Many responses may be worded differently but still be factually correct and helpful.\n",
        "\n",
        "Output format: {{\"score\": 1, \"reason\": \"explanation\"}} for correct responses\n",
        "               {{\"score\": 0, \"reason\": \"explanation\"}} for incorrect responses\n",
        "\n",
        "Score 1: The prediction is factually correct and helpful (even if expressed differently)\n",
        "Score 0: The prediction is factually wrong, misleading, or significantly unhelpful\"\"\"\n",
        "\n",
        "    # 限制文本长度以节省token\n",
        "    max_length = 1000\n",
        "    pred_text = prediction[:max_length]\n",
        "    ref_text = reference[:max_length]\n",
        "\n",
        "    # 添加省略号如果被截断\n",
        "    if len(prediction) > max_length:\n",
        "        pred_text += \" [truncated...]\"\n",
        "    if len(reference) > max_length:\n",
        "        ref_text += \" [truncated...]\"\n",
        "\n",
        "    user_message = f\"\"\"Task Type: {task_type}\n",
        "\n",
        "AI Prediction:\n",
        "{pred_text}\n",
        "\n",
        "Reference Answer:\n",
        "{ref_text}\n",
        "\n",
        "Is the AI prediction factually correct and helpful compared to the reference? Output JSON only.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0.1,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": judge_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ],\n",
        "            max_tokens=150\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        # 解析JSON\n",
        "        try:\n",
        "            # 查找JSON部分\n",
        "            start = content.find(\"{\")\n",
        "            end = content.rfind(\"}\") + 1\n",
        "\n",
        "            if start != -1 and end > start:\n",
        "                json_text = content[start:end]\n",
        "                result = json.loads(json_text)\n",
        "\n",
        "                score = int(result.get(\"score\", 0))\n",
        "                reason = result.get(\"reason\", \"no reason provided\")\n",
        "\n",
        "                return score, reason\n",
        "            else:\n",
        "                # 如果没有找到JSON，尝试从文本中提取\n",
        "                if \"score\" in content.lower():\n",
        "                    if \"1\" in content and (\"correct\" in content.lower() or \"accurate\" in content.lower()):\n",
        "                        return 1, \"extracted_positive\"\n",
        "                    else:\n",
        "                        return 0, \"extracted_negative\"\n",
        "                return 0, f\"parse_failed: {content}\"\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            # JSON解析失败，基于关键词判断\n",
        "            content_lower = content.lower()\n",
        "            if any(word in content_lower for word in [\"correct\", \"accurate\", \"helpful\", \"good\"]):\n",
        "                return 1, f\"keyword_positive: {content[:50]}\"\n",
        "            else:\n",
        "                return 0, f\"keyword_negative: {content[:50]}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return 0, f\"api_error: {str(e)}\"\n",
        "\n",
        "def evaluate_direct_gpt(input_path, sample_size=None, save_results=True):\n",
        "    \"\"\"直接用GPT评估所有样本\"\"\"\n",
        "\n",
        "    print(\"=== 直接GPT判断评估 ===\")\n",
        "    print(f\"输入文件: {input_path}\")\n",
        "\n",
        "    # 读取数据\n",
        "    data = read_jsonl_data(input_path)\n",
        "    print(f\"加载了 {len(data)} 个样本\")\n",
        "\n",
        "    # 采样\n",
        "    if sample_size and sample_size < len(data):\n",
        "        import random\n",
        "        data = random.sample(data, sample_size)\n",
        "        print(f\"采样 {sample_size} 个样本进行评估\")\n",
        "\n",
        "    results = []\n",
        "    correct_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    print(f\"\\n开始处理 {len(data)} 个样本...\")\n",
        "\n",
        "    for i, item in enumerate(data):\n",
        "        print(f\"处理样本 {i+1}/{len(data)}\", end=\"\")\n",
        "\n",
        "        pred = item['prediction']\n",
        "        ref = item['reference']\n",
        "        prompt = item['prompt']\n",
        "\n",
        "        # 直接调用GPT判断\n",
        "        score, reason = gpt_judge(pred, ref, prompt)\n",
        "\n",
        "        if score == 1:\n",
        "            correct_count += 1\n",
        "            print(\" ✓\")\n",
        "        else:\n",
        "            print(f\" ✗ ({reason[:30]}...)\")\n",
        "\n",
        "        if \"api_error\" in reason:\n",
        "            error_count += 1\n",
        "\n",
        "        results.append({\n",
        "            'index': i,\n",
        "            'prompt': prompt[:100] + \"...\" if len(prompt) > 100 else prompt,\n",
        "            'prediction': pred[:200] + \"...\" if len(pred) > 200 else pred,\n",
        "            'reference': ref[:200] + \"...\" if len(ref) > 200 else ref,\n",
        "            'gpt_score': score,\n",
        "            'gpt_reason': reason\n",
        "        })\n",
        "\n",
        "        # API限速\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        # 每10个样本显示进度\n",
        "        if (i + 1) % 10 == 0:\n",
        "            current_accuracy = correct_count / (i + 1)\n",
        "            print(f\"  进度: {i+1}/{len(data)}, 当前准确率: {current_accuracy:.3f}\")\n",
        "\n",
        "    # 计算最终结果\n",
        "    total_samples = len(results)\n",
        "    accuracy = correct_count / total_samples\n",
        "\n",
        "    print(f\"\\n=== 评估结果 ===\")\n",
        "    print(f\"总样本数: {total_samples}\")\n",
        "    print(f\"正确样本: {correct_count}\")\n",
        "    print(f\"错误样本: {total_samples - correct_count}\")\n",
        "    print(f\"API错误: {error_count}\")\n",
        "    print(f\"最终准确率: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "    # 显示一些示例\n",
        "    print(f\"\\n=== 正确样本示例 ===\")\n",
        "    correct_examples = [r for r in results if r['gpt_score'] == 1][:2]\n",
        "    for ex in correct_examples:\n",
        "        print(f\"预测: {ex['prediction']}\")\n",
        "        print(f\"参考: {ex['reference']}\")\n",
        "        print(f\"原因: {ex['gpt_reason']}\")\n",
        "        print()\n",
        "\n",
        "    print(f\"=== 错误样本示例 ===\")\n",
        "    incorrect_examples = [r for r in results if r['gpt_score'] == 0][:2]\n",
        "    for ex in incorrect_examples:\n",
        "        print(f\"预测: {ex['prediction']}\")\n",
        "        print(f\"参考: {ex['reference']}\")\n",
        "        print(f\"原因: {ex['gpt_reason']}\")\n",
        "        print()\n",
        "\n",
        "    # 保存结果\n",
        "    if save_results:\n",
        "        output_file = f\"gpt_direct_evaluation_results.json\"\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({\n",
        "                'summary': {\n",
        "                    'total_samples': total_samples,\n",
        "                    'correct_count': correct_count,\n",
        "                    'accuracy': accuracy,\n",
        "                    'api_errors': error_count\n",
        "                },\n",
        "                'detailed_results': results\n",
        "            }, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\\n结果已保存到: {output_file}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def quick_test(input_path, n_samples=3):\n",
        "    \"\"\"快速测试GPT调用\"\"\"\n",
        "    print(\"=== 快速GPT测试 ===\")\n",
        "\n",
        "    data = read_jsonl_data(input_path)\n",
        "    if not data:\n",
        "        print(\"无法读取数据\")\n",
        "        return\n",
        "\n",
        "    # 测试前几个样本\n",
        "    for i in range(min(n_samples, len(data))):\n",
        "        item = data[i]\n",
        "        print(f\"\\n--- 测试样本 {i+1} ---\")\n",
        "        print(f\"预测: {item['prediction'][:100]}...\")\n",
        "        print(f\"参考: {item['reference'][:100]}...\")\n",
        "\n",
        "        score, reason = gpt_judge(item['prediction'], item['reference'], item['prompt'])\n",
        "\n",
        "        print(f\"GPT评分: {score}\")\n",
        "        print(f\"原因: {reason}\")\n",
        "\n",
        "        time.sleep(1)  # 短暂暂停\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/sharegpttest/generated_predictions.jsonl\"\n",
        "\n",
        "    results = evaluate_direct_gpt(input_file, sample_size=None, save_results=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iWiYiznxIx0"
      },
      "source": [
        "vb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0kpBJkDRKBS",
        "outputId": "e8f247dd-a3d7-4f24-98e3-e2d07bd56256"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在加载数据...\n",
            "加载了 548 条数据\n",
            "正在清理预测结果...\n",
            "\n",
            "--- 样本 1 清理对比 ---\n",
            "原始: Tony the knowledge and authority rapport with the audience.\n",
            "5. Benefits:: presenter offers an a offer or service being are are, emphasizing how it wor...\n",
            "清理后: Tony the knowledge and authority rapport with the audience. 5. Benefits:: presenter offers an a offer or service being are are, emphasizing how it wor...\n",
            "\n",
            "--- 样本 2 清理对比 ---\n",
            "原始: Sure. You're right. We need to do a lot more work to make sure that we can provide good support for customers, and we need to do a lot more work to ma...\n",
            "清理后: Sure. You're right. We need to do a lot more work to make sure that we can provide good support for customers, and need do a lot more work make that c...\n",
            "\n",
            "--- 样本 3 清理对比 ---\n",
            "原始: To make the string \"This is a new {object} at {place}\" into a Map, you can use the `Map` class in Java. Here's how you can do it:\n",
            "\n",
            "1. First, you need ...\n",
            "清理后: To make the string \"This is a new {object} at {place}\" into a Map, you can use the `Map` class in Java. Here's how you can do it: 1. First, need to cr...\n",
            "\n",
            "--- 样本 4 清理对比 ---\n",
            "原始: The rewritten paragraph is as follows:\n",
            "\n",
            "By the grace of the gods, the arcane and enigmatic art of metaphorical language has been summoned forth to she...\n",
            "清理后: The rewritten paragraph is as follows: By the grace of gods, arcane and enigmatic art of metaphorical language has been summoned forth to shed light u...\n",
            "\n",
            "--- 样本 5 清理对比 ---\n",
            "原始: Based on the information you've provided, it seems like the variable `dummy` is being used in the `getline()` function to read the next line of input....\n",
            "清理后: Based on the information you've provided, it seems like the variable `dummy` is being used in `getline()` function to read next line of input. This is...\n",
            "\n",
            "改进了 514 个样本 (93.8%)\n",
            "\n",
            "正在重新计算评估指标...\n",
            "\n",
            "=== 清理前后对比 ===\n",
            "样本数量: 548\n",
            "\n",
            "指标              原始         清理后        改进        \n",
            "--------------------------------------------------\n",
            "EM              0.0000     0.0000     +0.0000\n",
            "F1              0.1944     0.1977     +0.0033\n",
            "Reranker        0.0474     0.0182     -0.0292\n",
            "\n",
            "=== 改进最明显的样本 ===\n",
            "\n",
            "样本 170 (改进 +0.508):\n",
            "原始: ```\n",
            "SELECT Region, SUM(Sales) AS Total_Sales\n",
            "FROM sales_transaction_summary\n",
            "WHERE Merchandise Week =...\n",
            "清理: ``` SELECT Region, SUM(Sales) AS Total_Sales FROM sales_transaction_summary WHERE Merchandise Week =...\n",
            "标签: ```\n",
            "SELECT Region, SUM(Sales) AS Total_Sales\n",
            "FROM sales_transaction_summary\n",
            "WHERE Merchandise Week =...\n",
            "\n",
            "样本 125 (改进 +0.309):\n",
            "原始: Sure, here's the corrected version of the sentence:\n",
            "\n",
            "\"Tell us about a film or TV series you watched ...\n",
            "清理: Sure, here's the corrected version of the sentence: \"Tell us about a film or TV series you watched r...\n",
            "标签: Sure, here are the corrected sentences:\n",
            "\n",
            "\"Tell us about a film or TV series you watched recently. Wh...\n",
            "\n",
            "样本 539 (改进 +0.280):\n",
            "原始: The title and headline for the article should be as follows:\n",
            "\n",
            "Title: South Korea Announces Launch of...\n",
            "清理: The title and headline for the article should be as follows: Title: South Korea Announces Launch of ...\n",
            "标签: 1. South Korea to Launch 6G Network Service Two Years Ahead of Schedule\n",
            "2. South Korea Aims to Secur...\n",
            "\n",
            "正在保存清理后的数据到: /content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/cleaned_predictionsvb.jsonl\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "class PredictionCleaner:\n",
        "    \"\"\"用于清理和改进预测结果的类\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_repetitions(text: str, max_repeat: int = 3) -> str:\n",
        "        \"\"\"移除过度重复的词汇\"\"\"\n",
        "        words = text.split()\n",
        "        if not words:\n",
        "            return text\n",
        "\n",
        "        cleaned_words = []\n",
        "        word_count = Counter()\n",
        "\n",
        "        for word in words:\n",
        "            word_lower = word.lower().strip('.,!?;:')\n",
        "            word_count[word_lower] += 1\n",
        "\n",
        "            # 如果这个词已经重复太多次，跳过\n",
        "            if word_count[word_lower] <= max_repeat:\n",
        "                cleaned_words.append(word)\n",
        "            elif len(cleaned_words) == 0:  # 保留至少一个词\n",
        "                cleaned_words.append(word)\n",
        "\n",
        "        return ' '.join(cleaned_words)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_format_tokens(text: str) -> str:\n",
        "        \"\"\"移除格式标记和错误的系统词汇\"\"\"\n",
        "        # 移除chat格式标记\n",
        "        text = re.sub(r'<\\|im_start\\|>.*?<\\|im_end\\|>', '', text, flags=re.DOTALL)\n",
        "        text = re.sub(r'<\\|.*?\\|>', '', text)\n",
        "\n",
        "        # 移除assistant相关词汇\n",
        "        text = re.sub(r'\\b(assistant|system|user)\\b', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 移除开头的重复名字（如 \"Tony Tonyassistant\"）\n",
        "        text = re.sub(r'^(\\w+)\\s*\\1\\s*(assistant|system)?\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 移除其他常见错误格式\n",
        "        text = re.sub(r'\\bhelpful assistant\\b', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_sentence_repetitions(text: str) -> str:\n",
        "        \"\"\"移除重复的句子\"\"\"\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        seen_sentences = set()\n",
        "        unique_sentences = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_clean = sentence.strip().lower()\n",
        "            if sentence_clean and sentence_clean not in seen_sentences:\n",
        "                seen_sentences.add(sentence_clean)\n",
        "                unique_sentences.append(sentence.strip())\n",
        "\n",
        "        return '. '.join(s for s in unique_sentences if s) + ('.' if unique_sentences else '')\n",
        "\n",
        "    @staticmethod\n",
        "    def fix_common_errors(text: str) -> str:\n",
        "        \"\"\"修复常见错误\"\"\"\n",
        "        # 修复粘连的词（如 \"Tonyassistant\" -> \"Tony\"）\n",
        "        text = re.sub(r'(\\w+)assistant', r'\\1', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'(\\w+)system', r'\\1', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 修复多余的换行和空格\n",
        "        text = re.sub(r'\\n+', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # 修复开头的错误（移除开头重复的词）\n",
        "        words = text.split()\n",
        "        if len(words) > 1 and words[0].lower() == words[1].lower():\n",
        "            text = ' '.join(words[1:])\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    @classmethod\n",
        "    def clean_prediction(cls, text: str) -> str:\n",
        "        \"\"\"综合清理预测文本\"\"\"\n",
        "        # 按步骤清理\n",
        "        text = cls.remove_format_tokens(text)\n",
        "        text = cls.fix_common_errors(text)\n",
        "        text = cls.remove_repetitions(text, max_repeat=2)  # 更严格的重复控制\n",
        "        text = cls.remove_sentence_repetitions(text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "def clean_and_evaluate(input_file: str, output_file: str = None):\n",
        "    \"\"\"清理预测并重新评估\"\"\"\n",
        "\n",
        "    # 加载数据\n",
        "    print(\"正在加载数据...\")\n",
        "    data = []\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line.strip()))\n",
        "\n",
        "    print(f\"加载了 {len(data)} 条数据\")\n",
        "\n",
        "    # 清理预测\n",
        "    print(\"正在清理预测结果...\")\n",
        "    cleaned_data = []\n",
        "    improvement_count = 0\n",
        "\n",
        "    for i, item in enumerate(data):\n",
        "        original_pred = item['predict']\n",
        "        cleaned_pred = PredictionCleaner.clean_prediction(original_pred)\n",
        "\n",
        "        # 检查是否有改进\n",
        "        if cleaned_pred != original_pred:\n",
        "            improvement_count += 1\n",
        "\n",
        "        cleaned_item = item.copy()\n",
        "        cleaned_item['original_predict'] = original_pred\n",
        "        cleaned_item['predict'] = cleaned_pred\n",
        "        cleaned_data.append(cleaned_item)\n",
        "\n",
        "        # 显示前几个清理示例\n",
        "        if i < 5:\n",
        "            print(f\"\\n--- 样本 {i+1} 清理对比 ---\")\n",
        "            print(f\"原始: {original_pred[:150]}...\")\n",
        "            print(f\"清理后: {cleaned_pred[:150]}...\")\n",
        "\n",
        "    print(f\"\\n改进了 {improvement_count} 个样本 ({improvement_count/len(data)*100:.1f}%)\")\n",
        "\n",
        "    # 重新计算评估指标\n",
        "    print(\"\\n正在重新计算评估指标...\")\n",
        "\n",
        "    # 评估函数\n",
        "    def normalize_answer(s: str) -> str:\n",
        "        def remove_articles(text):\n",
        "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "        def white_space_fix(text):\n",
        "            return ' '.join(text.split())\n",
        "        def remove_punc(text):\n",
        "            exclude = set(string.punctuation)\n",
        "            return ''.join(ch for ch in text if ch not in exclude)\n",
        "        def lower(text):\n",
        "            return text.lower()\n",
        "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "    def exact_match(prediction: str, ground_truth: str) -> float:\n",
        "        return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "    def f1_score(prediction: str, ground_truth: str) -> float:\n",
        "        pred_tokens = normalize_answer(prediction).split()\n",
        "        truth_tokens = normalize_answer(ground_truth).split()\n",
        "\n",
        "        if len(pred_tokens) == 0 and len(truth_tokens) == 0:\n",
        "            return 1.0\n",
        "        if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        common = Counter(pred_tokens) & Counter(truth_tokens)\n",
        "        num_same = sum(common.values())\n",
        "\n",
        "        precision = num_same / len(pred_tokens)\n",
        "        recall = num_same / len(truth_tokens)\n",
        "\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "\n",
        "        return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    def partial_match_score(prediction: str, ground_truth: str, threshold: float = 0.7) -> float:\n",
        "        \"\"\"计算部分匹配分数，用于改进的Reranker\"\"\"\n",
        "        f1 = f1_score(prediction, ground_truth)\n",
        "        return 1.0 if f1 >= threshold else 0.0\n",
        "\n",
        "    # 计算原始分数和清理后分数\n",
        "    original_em_scores = []\n",
        "    original_f1_scores = []\n",
        "    original_reranker_scores = []\n",
        "\n",
        "    cleaned_em_scores = []\n",
        "    cleaned_f1_scores = []\n",
        "    cleaned_reranker_scores = []\n",
        "\n",
        "    for item in cleaned_data:\n",
        "        original_pred = item['original_predict']\n",
        "        cleaned_pred = item['predict']\n",
        "        label = item['label']\n",
        "\n",
        "        # 原始分数\n",
        "        original_em_scores.append(exact_match(original_pred, label))\n",
        "        original_f1_scores.append(f1_score(original_pred, label))\n",
        "        original_reranker_scores.append(partial_match_score(original_pred, label, 0.5))\n",
        "\n",
        "        # 清理后分数\n",
        "        cleaned_em_scores.append(exact_match(cleaned_pred, label))\n",
        "        cleaned_f1_scores.append(f1_score(cleaned_pred, label))\n",
        "        cleaned_reranker_scores.append(partial_match_score(cleaned_pred, label, 0.5))\n",
        "\n",
        "    # 计算平均分数\n",
        "    original_results = {\n",
        "        'EM': np.mean(original_em_scores),\n",
        "        'F1': np.mean(original_f1_scores),\n",
        "        'Reranker': np.mean(original_reranker_scores)\n",
        "    }\n",
        "\n",
        "    cleaned_results = {\n",
        "        'EM': np.mean(cleaned_em_scores),\n",
        "        'F1': np.mean(cleaned_f1_scores),\n",
        "        'Reranker': np.mean(cleaned_reranker_scores)\n",
        "    }\n",
        "\n",
        "    # 打印对比结果\n",
        "    print(\"\\n=== 清理前后对比 ===\")\n",
        "    print(f\"样本数量: {len(data)}\")\n",
        "    print(f\"\\n{'指标':<15} {'原始':<10} {'清理后':<10} {'改进':<10}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for metric in ['EM', 'F1', 'Reranker']:\n",
        "        original = original_results[metric]\n",
        "        cleaned = cleaned_results[metric]\n",
        "        improvement = cleaned - original\n",
        "        print(f\"{metric:<15} {original:<10.4f} {cleaned:<10.4f} {improvement:>+.4f}\")\n",
        "\n",
        "    # 找出改进最明显的样本\n",
        "    print(\"\\n=== 改进最明显的样本 ===\")\n",
        "    improvements = []\n",
        "    for i, item in enumerate(cleaned_data):\n",
        "        original_f1 = f1_score(item['original_predict'], item['label'])\n",
        "        cleaned_f1 = f1_score(item['predict'], item['label'])\n",
        "        improvement = cleaned_f1 - original_f1\n",
        "        if improvement > 0.1:  # 改进超过0.1的样本\n",
        "            improvements.append((i, improvement, item))\n",
        "\n",
        "    improvements.sort(key=lambda x: x[1], reverse=True)\n",
        "    for i, (idx, improvement, item) in enumerate(improvements[:3]):\n",
        "        print(f\"\\n样本 {idx} (改进 +{improvement:.3f}):\")\n",
        "        print(f\"原始: {item['original_predict'][:100]}...\")\n",
        "        print(f\"清理: {item['predict'][:100]}...\")\n",
        "        print(f\"标签: {item['label'][:100]}...\")\n",
        "\n",
        "    # 保存清理后的数据\n",
        "    if output_file:\n",
        "        print(f\"\\n正在保存清理后的数据到: {output_file}\")\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            for item in cleaned_data:\n",
        "                # 只保存清理后的predict，移除original_predict以节省空间\n",
        "                save_item = {k: v for k, v in item.items() if k != 'original_predict'}\n",
        "                f.write(json.dumps(save_item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    return original_results, cleaned_results\n",
        "\n",
        "# 运行清理和评估\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/ultrachatvb/generated_predictions.jsonl\"\n",
        "    output_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/cleaned_predictionsvb.jsonl\"\n",
        "\n",
        "    original_results, cleaned_results = clean_and_evaluate(input_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGzXFjO-RfNo",
        "outputId": "7c029058-6b63-44a4-d38f-e65ca5065cc8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "class SmartCleaner:\n",
        "    \"\"\"更智能的预测清理器\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_severe_repetition(text: str) -> bool:\n",
        "        \"\"\"检测严重的重复问题\"\"\"\n",
        "        words = text.split()\n",
        "        if len(words) < 5:\n",
        "            return False\n",
        "\n",
        "        # 检查是否有词汇重复超过总词数的50%\n",
        "        word_counts = Counter(words)\n",
        "        max_count = max(word_counts.values()) if word_counts else 0\n",
        "        repetition_ratio = max_count / len(words)\n",
        "\n",
        "        return repetition_ratio > 0.5\n",
        "\n",
        "    @staticmethod\n",
        "    def is_garbage_output(text: str) -> bool:\n",
        "        \"\"\"判断是否为垃圾输出\"\"\"\n",
        "        # 检查各种垃圾输出模式\n",
        "        text_lower = text.lower().strip()\n",
        "\n",
        "        # 完全重复的单词\n",
        "        words = text.split()\n",
        "        if len(words) > 10:\n",
        "            unique_words = set(words)\n",
        "            if len(unique_words) <= 3:  # 只有很少的不同词汇\n",
        "                return True\n",
        "\n",
        "        # 检查是否主要是重复字符\n",
        "        if len(text) > 50:\n",
        "            char_counts = Counter(text.replace(' ', '').replace('\\n', ''))\n",
        "            if char_counts and max(char_counts.values()) > len(text) * 0.4:\n",
        "                return True\n",
        "\n",
        "        # 检查是否包含明显的格式错误\n",
        "        error_patterns = [\n",
        "            r'^(\\w{1,4})\\1{10,}',  # 短词重复很多次\n",
        "            r'(assistant|system|user)\\s*\\1',  # 系统词重复\n",
        "        ]\n",
        "\n",
        "        for pattern in error_patterns:\n",
        "            if re.search(pattern, text_lower):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def smart_repetition_fix(text: str) -> str:\n",
        "        \"\"\"智能修复重复问题\"\"\"\n",
        "        # 修复开头的重复问题\n",
        "        text = re.sub(r'^(\\w+)\\s*\\1\\s*(assistant|system)?\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 修复句子级别的重复\n",
        "        sentences = re.split(r'([.!?]+)', text)\n",
        "        cleaned_sentences = []\n",
        "        seen_sentences = set()\n",
        "\n",
        "        for i in range(0, len(sentences), 2):\n",
        "            if i < len(sentences):\n",
        "                sentence = sentences[i].strip()\n",
        "                sentence_key = re.sub(r'\\s+', ' ', sentence.lower())\n",
        "\n",
        "                if sentence_key and sentence_key not in seen_sentences and len(sentence) > 10:\n",
        "                    seen_sentences.add(sentence_key)\n",
        "                    cleaned_sentences.append(sentence)\n",
        "                    if i + 1 < len(sentences):\n",
        "                        cleaned_sentences.append(sentences[i + 1])  # 保留标点\n",
        "\n",
        "        result = ''.join(cleaned_sentences).strip()\n",
        "\n",
        "        # 如果清理后太短，使用原文\n",
        "        if len(result) < len(text) * 0.3:\n",
        "            return text\n",
        "\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def conservative_clean(text: str) -> str:\n",
        "        \"\"\"保守的清理策略\"\"\"\n",
        "        original_text = text\n",
        "\n",
        "        # 只修复明显的问题\n",
        "        # 1. 移除格式标记\n",
        "        text = re.sub(r'<\\|im_start\\|>.*?<\\|im_end\\|>', '', text, flags=re.DOTALL)\n",
        "\n",
        "        # 2. 修复开头的重复词和格式错误\n",
        "        text = re.sub(r'^(\\w+)\\s*\\1\\s*assistant\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'^(\\w+)\\s*assistant\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 3. 移除standalone的assistant/system/user\n",
        "        text = re.sub(r'\\b(assistant|system|user)\\s+', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 4. 修复多余空格\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # 如果清理后明显变差，返回原文\n",
        "        if len(text) < len(original_text) * 0.7:\n",
        "            return original_text\n",
        "\n",
        "        return text\n",
        "\n",
        "    @classmethod\n",
        "    def process_prediction(cls, text: str) -> Tuple[str, str]:\n",
        "        \"\"\"\n",
        "        处理预测文本\n",
        "        返回: (清理后的文本, 处理策略)\n",
        "        \"\"\"\n",
        "        # 检查是否为严重的垃圾输出\n",
        "        if cls.is_garbage_output(text):\n",
        "            # 对于垃圾输出，尝试从中提取有意义的部分\n",
        "            words = text.split()\n",
        "            if words:\n",
        "                # 尝试找到第一个正常的词开始\n",
        "                meaningful_start = 0\n",
        "                for i, word in enumerate(words):\n",
        "                    if len(word) > 2 and word.lower() not in ['to', 'the', 'and', 'or', 'but']:\n",
        "                        meaningful_start = i\n",
        "                        break\n",
        "\n",
        "                # 取前面一些正常词汇\n",
        "                if meaningful_start < len(words) - 5:\n",
        "                    reconstructed = ' '.join(words[meaningful_start:meaningful_start + 20])\n",
        "                    return reconstructed, \"garbage_reconstruction\"\n",
        "\n",
        "            return text, \"garbage_kept\"\n",
        "\n",
        "        # 检查严重重复\n",
        "        elif cls.detect_severe_repetition(text):\n",
        "            cleaned = cls.smart_repetition_fix(text)\n",
        "            return cleaned, \"repetition_fix\"\n",
        "\n",
        "        # 保守清理\n",
        "        else:\n",
        "            cleaned = cls.conservative_clean(text)\n",
        "            return cleaned, \"conservative\"\n",
        "\n",
        "def advanced_evaluation(input_file: str, output_file: str = None):\n",
        "    \"\"\"高级评估和清理\"\"\"\n",
        "\n",
        "    # 加载数据\n",
        "    print(\"正在加载数据...\")\n",
        "    data = []\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line.strip()))\n",
        "\n",
        "    print(f\"加载了 {len(data)} 条数据\")\n",
        "\n",
        "    # 分析和清理\n",
        "    print(\"正在分析和清理预测...\")\n",
        "\n",
        "    strategy_counts = Counter()\n",
        "    cleaned_data = []\n",
        "    severe_issues = []\n",
        "\n",
        "    for i, item in enumerate(data):\n",
        "        original_pred = item['predict']\n",
        "        cleaned_pred, strategy = SmartCleaner.process_prediction(original_pred)\n",
        "\n",
        "        strategy_counts[strategy] += 1\n",
        "\n",
        "        # 记录严重问题的样本\n",
        "        if strategy in [\"garbage_reconstruction\", \"garbage_kept\"]:\n",
        "            severe_issues.append({\n",
        "                'index': i,\n",
        "                'strategy': strategy,\n",
        "                'original': original_pred[:200] + \"...\" if len(original_pred) > 200 else original_pred,\n",
        "                'cleaned': cleaned_pred[:200] + \"...\" if len(cleaned_pred) > 200 else cleaned_pred,\n",
        "                'label': item['label'][:200] + \"...\" if len(item['label']) > 200 else item['label']\n",
        "            })\n",
        "\n",
        "        item_copy = item.copy()\n",
        "        item_copy['original_predict'] = original_pred\n",
        "        item_copy['predict'] = cleaned_pred\n",
        "        item_copy['clean_strategy'] = strategy\n",
        "        cleaned_data.append(item_copy)\n",
        "\n",
        "    print(f\"\\n清理策略统计:\")\n",
        "    for strategy, count in strategy_counts.items():\n",
        "        print(f\"  {strategy}: {count} 个样本 ({count/len(data)*100:.1f}%)\")\n",
        "\n",
        "    # 显示严重问题样本\n",
        "    if severe_issues:\n",
        "        print(f\"\\n=== 发现 {len(severe_issues)} 个严重问题样本 ===\")\n",
        "        for i, issue in enumerate(severe_issues[:3]):\n",
        "            print(f\"\\n样本 {issue['index']} ({issue['strategy']}):\")\n",
        "            print(f\"原始: {issue['original']}\")\n",
        "            print(f\"清理: {issue['cleaned']}\")\n",
        "            print(f\"标签: {issue['label']}\")\n",
        "\n",
        "    # 重新评估\n",
        "    print(\"\\n正在重新计算评估指标...\")\n",
        "\n",
        "    def normalize_answer(s: str) -> str:\n",
        "        def remove_articles(text):\n",
        "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "        def white_space_fix(text):\n",
        "            return ' '.join(text.split())\n",
        "        def remove_punc(text):\n",
        "            exclude = set(string.punctuation)\n",
        "            return ''.join(ch for ch in text if ch not in exclude)\n",
        "        def lower(text):\n",
        "            return text.lower()\n",
        "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "    def exact_match(prediction: str, ground_truth: str) -> float:\n",
        "        return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "    def f1_score(prediction: str, ground_truth: str) -> float:\n",
        "        pred_tokens = normalize_answer(prediction).split()\n",
        "        truth_tokens = normalize_answer(ground_truth).split()\n",
        "\n",
        "        if len(pred_tokens) == 0 and len(truth_tokens) == 0:\n",
        "            return 1.0\n",
        "        if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        common = Counter(pred_tokens) & Counter(truth_tokens)\n",
        "        num_same = sum(common.values())\n",
        "\n",
        "        if num_same == 0:\n",
        "            return 0.0\n",
        "\n",
        "        precision = num_same / len(pred_tokens)\n",
        "        recall = num_same / len(truth_tokens)\n",
        "\n",
        "        return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    def smart_reranker(prediction: str, ground_truth: str) -> float:\n",
        "        \"\"\"智能Reranker，使用多个标准\"\"\"\n",
        "        # 标准1: F1分数高于阈值\n",
        "        f1 = f1_score(prediction, ground_truth)\n",
        "        if f1 >= 0.6:\n",
        "            return 1.0\n",
        "\n",
        "        # 标准2: 关键词匹配\n",
        "        pred_words = set(normalize_answer(prediction).split())\n",
        "        truth_words = set(normalize_answer(ground_truth).split())\n",
        "\n",
        "        if len(truth_words) > 0:\n",
        "            keyword_overlap = len(pred_words & truth_words) / len(truth_words)\n",
        "            if keyword_overlap >= 0.4:\n",
        "                return 1.0\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    # 计算分数\n",
        "    strategies = ['conservative', 'repetition_fix', 'garbage_reconstruction', 'garbage_kept']\n",
        "    results_by_strategy = {}\n",
        "\n",
        "    for strategy in strategies:\n",
        "        strategy_items = [item for item in cleaned_data if item['clean_strategy'] == strategy]\n",
        "        if not strategy_items:\n",
        "            continue\n",
        "\n",
        "        em_scores = []\n",
        "        f1_scores = []\n",
        "        reranker_scores = []\n",
        "\n",
        "        for item in strategy_items:\n",
        "            pred = item['predict']\n",
        "            label = item['label']\n",
        "\n",
        "            em_scores.append(exact_match(pred, label))\n",
        "            f1_scores.append(f1_score(pred, label))\n",
        "            reranker_scores.append(smart_reranker(pred, label))\n",
        "\n",
        "        results_by_strategy[strategy] = {\n",
        "            'count': len(strategy_items),\n",
        "            'EM': np.mean(em_scores),\n",
        "            'F1': np.mean(f1_scores),\n",
        "            'Reranker': np.mean(reranker_scores)\n",
        "        }\n",
        "\n",
        "    # 计算总体分数\n",
        "    all_em_scores = []\n",
        "    all_f1_scores = []\n",
        "    all_reranker_scores = []\n",
        "    all_original_f1_scores = []\n",
        "\n",
        "    for item in cleaned_data:\n",
        "        pred = item['predict']\n",
        "        orig_pred = item['original_predict']\n",
        "        label = item['label']\n",
        "\n",
        "        all_em_scores.append(exact_match(pred, label))\n",
        "        all_f1_scores.append(f1_score(pred, label))\n",
        "        all_reranker_scores.append(smart_reranker(pred, label))\n",
        "        all_original_f1_scores.append(f1_score(orig_pred, label))\n",
        "\n",
        "    overall_results = {\n",
        "        'EM': np.mean(all_em_scores),\n",
        "        'F1': np.mean(all_f1_scores),\n",
        "        'Reranker': np.mean(all_reranker_scores)\n",
        "    }\n",
        "\n",
        "    original_f1 = np.mean(all_original_f1_scores)\n",
        "\n",
        "    # 打印结果\n",
        "    print(f\"\\n=== 智能清理结果 ===\")\n",
        "    print(f\"总体改进:\")\n",
        "    print(f\"  原始F1: {original_f1:.4f}\")\n",
        "    print(f\"  清理后F1: {overall_results['F1']:.4f} (改进: {overall_results['F1'] - original_f1:+.4f})\")\n",
        "    print(f\"  EM: {overall_results['EM']:.4f}\")\n",
        "    print(f\"  智能Reranker: {overall_results['Reranker']:.4f}\")\n",
        "\n",
        "    print(f\"\\n按策略分类的结果:\")\n",
        "    for strategy, results in results_by_strategy.items():\n",
        "        print(f\"  {strategy} ({results['count']} 样本):\")\n",
        "        print(f\"    EM: {results['EM']:.4f}, F1: {results['F1']:.4f}, Reranker: {results['Reranker']:.4f}\")\n",
        "\n",
        "    # 保存结果\n",
        "    if output_file:\n",
        "        print(f\"\\n正在保存清理后的数据到: {output_file}\")\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            for item in cleaned_data:\n",
        "                # 保存清理后的数据，包含策略信息\n",
        "                save_item = {\n",
        "                    'prompt': item['prompt'],\n",
        "                    'predict': item['predict'],\n",
        "                    'label': item['label'],\n",
        "                    'clean_strategy': item['clean_strategy']\n",
        "                }\n",
        "                f.write(json.dumps(save_item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    return overall_results, results_by_strategy\n",
        "\n",
        "# 运行智能清理\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/sharegpttest/generated_predictions.jsonl\"\n",
        "    output_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/smart_cleaned_predictions.jsonl\"\n",
        "\n",
        "    overall_results, strategy_results = advanced_evaluation(input_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8KYke9uR1ja",
        "outputId": "0141c04f-0234-4fd5-9b72-39b3bd6fa0de"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# 设置API Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "client = OpenAI()\n",
        "\n",
        "def read_jsonl_data(path):\n",
        "    \"\"\"读取JSONL数据\"\"\"\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                item = json.loads(line)\n",
        "                data.append({\n",
        "                    'prompt': item.get('prompt', ''),\n",
        "                    'prediction': item.get('predict', ''),\n",
        "                    'reference': item.get('label', '')\n",
        "                })\n",
        "    return data\n",
        "\n",
        "def gpt_judge(prediction, reference, prompt=\"\"):\n",
        "    \"\"\"直接用GPT判断\"\"\"\n",
        "\n",
        "    # 识别任务类型\n",
        "    task_type = \"general conversation\"\n",
        "    if prompt:\n",
        "        prompt_lower = prompt.lower()\n",
        "        if any(word in prompt_lower for word in ['code', 'programming', 'function', 'algorithm']):\n",
        "            task_type = \"coding\"\n",
        "        elif any(word in prompt_lower for word in ['explain', 'what is', 'describe', 'definition']):\n",
        "            task_type = \"explanation\"\n",
        "        elif any(word in prompt_lower for word in ['list', 'steps', 'how to', 'instruction']):\n",
        "            task_type = \"instruction\"\n",
        "        elif any(word in prompt_lower for word in ['analyze', 'compare', 'evaluate']):\n",
        "            task_type = \"analysis\"\n",
        "\n",
        "    judge_prompt = f\"\"\"You are evaluating an AI assistant's response for a {task_type} task.\n",
        "\n",
        "Evaluation Guidelines:\n",
        "- Focus on factual correctness and helpfulness\n",
        "- Accept different but semantically equivalent answers\n",
        "- Ignore formatting, style, and minor wording differences\n",
        "- Consider the core message and information accuracy\n",
        "- Be reasonably lenient - if the prediction conveys the same key information as the reference, it should be considered correct\n",
        "- For explanations: focus on conceptual accuracy rather than exact phrasing\n",
        "- For instructions: check if the steps achieve the same goal\n",
        "- For analysis: evaluate logical reasoning and key conclusions\n",
        "\n",
        "Important: Many responses may be worded differently but still be factually correct and helpful.\n",
        "\n",
        "Output format: {{\"score\": 1, \"reason\": \"explanation\"}} for correct responses\n",
        "               {{\"score\": 0, \"reason\": \"explanation\"}} for incorrect responses\n",
        "\n",
        "Score 1: The prediction is factually correct and helpful (even if expressed differently)\n",
        "Score 0: The prediction is factually wrong, misleading, or significantly unhelpful\"\"\"\n",
        "\n",
        "    # 限制文本长度以节省token\n",
        "    max_length = 1000\n",
        "    pred_text = prediction[:max_length]\n",
        "    ref_text = reference[:max_length]\n",
        "\n",
        "    # 添加省略号如果被截断\n",
        "    if len(prediction) > max_length:\n",
        "        pred_text += \" [truncated...]\"\n",
        "    if len(reference) > max_length:\n",
        "        ref_text += \" [truncated...]\"\n",
        "\n",
        "    user_message = f\"\"\"Task Type: {task_type}\n",
        "\n",
        "AI Prediction:\n",
        "{pred_text}\n",
        "\n",
        "Reference Answer:\n",
        "{ref_text}\n",
        "\n",
        "Is the AI prediction factually correct and helpful compared to the reference? Output JSON only.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0.1,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": judge_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ],\n",
        "            max_tokens=150\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        # 解析JSON\n",
        "        try:\n",
        "            # 查找JSON部分\n",
        "            start = content.find(\"{\")\n",
        "            end = content.rfind(\"}\") + 1\n",
        "\n",
        "            if start != -1 and end > start:\n",
        "                json_text = content[start:end]\n",
        "                result = json.loads(json_text)\n",
        "\n",
        "                score = int(result.get(\"score\", 0))\n",
        "                reason = result.get(\"reason\", \"no reason provided\")\n",
        "\n",
        "                return score, reason\n",
        "            else:\n",
        "                # 如果没有找到JSON，尝试从文本中提取\n",
        "                if \"score\" in content.lower():\n",
        "                    if \"1\" in content and (\"correct\" in content.lower() or \"accurate\" in content.lower()):\n",
        "                        return 1, \"extracted_positive\"\n",
        "                    else:\n",
        "                        return 0, \"extracted_negative\"\n",
        "                return 0, f\"parse_failed: {content}\"\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            # JSON解析失败，基于关键词判断\n",
        "            content_lower = content.lower()\n",
        "            if any(word in content_lower for word in [\"correct\", \"accurate\", \"helpful\", \"good\"]):\n",
        "                return 1, f\"keyword_positive: {content[:50]}\"\n",
        "            else:\n",
        "                return 0, f\"keyword_negative: {content[:50]}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return 0, f\"api_error: {str(e)}\"\n",
        "\n",
        "def evaluate_direct_gpt(input_path, sample_size=None, save_results=True):\n",
        "    \"\"\"直接用GPT评估所有样本\"\"\"\n",
        "\n",
        "    print(\"=== 直接GPT判断评估 ===\")\n",
        "    print(f\"输入文件: {input_path}\")\n",
        "\n",
        "    # 读取数据\n",
        "    data = read_jsonl_data(input_path)\n",
        "    print(f\"加载了 {len(data)} 个样本\")\n",
        "\n",
        "    # 采样\n",
        "    if sample_size and sample_size < len(data):\n",
        "        import random\n",
        "        data = random.sample(data, sample_size)\n",
        "        print(f\"采样 {sample_size} 个样本进行评估\")\n",
        "\n",
        "    results = []\n",
        "    correct_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    print(f\"\\n开始处理 {len(data)} 个样本...\")\n",
        "\n",
        "    for i, item in enumerate(data):\n",
        "        print(f\"处理样本 {i+1}/{len(data)}\", end=\"\")\n",
        "\n",
        "        pred = item['prediction']\n",
        "        ref = item['reference']\n",
        "        prompt = item['prompt']\n",
        "\n",
        "        # 直接调用GPT判断\n",
        "        score, reason = gpt_judge(pred, ref, prompt)\n",
        "\n",
        "        if score == 1:\n",
        "            correct_count += 1\n",
        "            print(\" ✓\")\n",
        "        else:\n",
        "            print(f\" ✗ ({reason[:30]}...)\")\n",
        "\n",
        "        if \"api_error\" in reason:\n",
        "            error_count += 1\n",
        "\n",
        "        results.append({\n",
        "            'index': i,\n",
        "            'prompt': prompt[:100] + \"...\" if len(prompt) > 100 else prompt,\n",
        "            'prediction': pred[:200] + \"...\" if len(pred) > 200 else pred,\n",
        "            'reference': ref[:200] + \"...\" if len(ref) > 200 else ref,\n",
        "            'gpt_score': score,\n",
        "            'gpt_reason': reason\n",
        "        })\n",
        "\n",
        "        # API限速\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        # 每10个样本显示进度\n",
        "        if (i + 1) % 10 == 0:\n",
        "            current_accuracy = correct_count / (i + 1)\n",
        "            print(f\"  进度: {i+1}/{len(data)}, 当前准确率: {current_accuracy:.3f}\")\n",
        "\n",
        "    # 计算最终结果\n",
        "    total_samples = len(results)\n",
        "    accuracy = correct_count / total_samples\n",
        "\n",
        "    print(f\"\\n=== 评估结果 ===\")\n",
        "    print(f\"总样本数: {total_samples}\")\n",
        "    print(f\"正确样本: {correct_count}\")\n",
        "    print(f\"错误样本: {total_samples - correct_count}\")\n",
        "    print(f\"API错误: {error_count}\")\n",
        "    print(f\"最终准确率: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "    # 显示一些示例\n",
        "    print(f\"\\n=== 正确样本示例 ===\")\n",
        "    correct_examples = [r for r in results if r['gpt_score'] == 1][:2]\n",
        "    for ex in correct_examples:\n",
        "        print(f\"预测: {ex['prediction']}\")\n",
        "        print(f\"参考: {ex['reference']}\")\n",
        "        print(f\"原因: {ex['gpt_reason']}\")\n",
        "        print()\n",
        "\n",
        "    print(f\"=== 错误样本示例 ===\")\n",
        "    incorrect_examples = [r for r in results if r['gpt_score'] == 0][:2]\n",
        "    for ex in incorrect_examples:\n",
        "        print(f\"预测: {ex['prediction']}\")\n",
        "        print(f\"参考: {ex['reference']}\")\n",
        "        print(f\"原因: {ex['gpt_reason']}\")\n",
        "        print()\n",
        "\n",
        "    # 保存结果\n",
        "    if save_results:\n",
        "        output_file = f\"gpt_direct_evaluation_results.json\"\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({\n",
        "                'summary': {\n",
        "                    'total_samples': total_samples,\n",
        "                    'correct_count': correct_count,\n",
        "                    'accuracy': accuracy,\n",
        "                    'api_errors': error_count\n",
        "                },\n",
        "                'detailed_results': results\n",
        "            }, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\\n结果已保存到: {output_file}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def quick_test(input_path, n_samples=3):\n",
        "    \"\"\"快速测试GPT调用\"\"\"\n",
        "    print(\"=== 快速GPT测试 ===\")\n",
        "\n",
        "    data = read_jsonl_data(input_path)\n",
        "    if not data:\n",
        "        print(\"无法读取数据\")\n",
        "        return\n",
        "\n",
        "    # 测试前几个样本\n",
        "    for i in range(min(n_samples, len(data))):\n",
        "        item = data[i]\n",
        "        print(f\"\\n--- 测试样本 {i+1} ---\")\n",
        "        print(f\"预测: {item['prediction'][:100]}...\")\n",
        "        print(f\"参考: {item['reference'][:100]}...\")\n",
        "\n",
        "        score, reason = gpt_judge(item['prediction'], item['reference'], item['prompt'])\n",
        "\n",
        "        print(f\"GPT评分: {score}\")\n",
        "        print(f\"原因: {reason}\")\n",
        "\n",
        "        time.sleep(1)  # 短暂暂停\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/MyDrive/llama_saves/Qwen2-VL-2B/lora/sharegpttest/generated_predictions.jsonl\"\n",
        "\n",
        "    results = evaluate_direct_gpt(input_file, sample_size=None, save_results=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
